{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "9d968df4de6b44ed8fd5895a0cc73bec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c8beba62315a48bc85e4b72cf481c5a7",
              "IPY_MODEL_9dff21869d2f4dde851d59a1b8c6d4b0",
              "IPY_MODEL_f487bee11a34450e899df6ff4b0ae0cd"
            ],
            "layout": "IPY_MODEL_9a266261140a4b028271de068da2c186"
          }
        },
        "c8beba62315a48bc85e4b72cf481c5a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c0e5a9ffa19e4a78ac7fd1a84e2f2809",
            "placeholder": "​",
            "style": "IPY_MODEL_95b043ea17704d85974eee44a87dbd99",
            "value": "Dl Completed...: 100%"
          }
        },
        "9dff21869d2f4dde851d59a1b8c6d4b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cd4f12e9bc54438bb09a99e3c11be3c5",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5a27e230024e48ecaa1416ec5edf6ea3",
            "value": 1
          }
        },
        "f487bee11a34450e899df6ff4b0ae0cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f039ef6526da41e8b0917487b38b0f02",
            "placeholder": "​",
            "style": "IPY_MODEL_2d5677f122d04591b3cf25f31060febf",
            "value": " 1/1 [00:10&lt;00:00, 10.69s/ url]"
          }
        },
        "9a266261140a4b028271de068da2c186": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c0e5a9ffa19e4a78ac7fd1a84e2f2809": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "95b043ea17704d85974eee44a87dbd99": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cd4f12e9bc54438bb09a99e3c11be3c5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "5a27e230024e48ecaa1416ec5edf6ea3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f039ef6526da41e8b0917487b38b0f02": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2d5677f122d04591b3cf25f31060febf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b59b71596d7b4ef1912e5b230e21b5a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_430d5d35461640518cc67d14c43abd41",
              "IPY_MODEL_9f0ee101b83a419489a32ba6396c412c",
              "IPY_MODEL_5722a7a24071492e8db5abfcaa8467ae"
            ],
            "layout": "IPY_MODEL_c3fc5afed0f2425393206f775ec435b1"
          }
        },
        "430d5d35461640518cc67d14c43abd41": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6e1661af9e05418b89ddee67f351ec57",
            "placeholder": "​",
            "style": "IPY_MODEL_9216f972148044a6978efdc864784000",
            "value": "Dl Size...: 100%"
          }
        },
        "9f0ee101b83a419489a32ba6396c412c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f0c6af735b614e67a56169515b9416b5",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ef2ed027e80b49ad82dfebe139ff10ed",
            "value": 1
          }
        },
        "5722a7a24071492e8db5abfcaa8467ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3374ce91c1df4014b48e388a454a5e5d",
            "placeholder": "​",
            "style": "IPY_MODEL_ff234810a9d346699ee6151dfb9d87f1",
            "value": " 786/786 [00:10&lt;00:00, 73.72 MiB/s]"
          }
        },
        "c3fc5afed0f2425393206f775ec435b1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6e1661af9e05418b89ddee67f351ec57": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9216f972148044a6978efdc864784000": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f0c6af735b614e67a56169515b9416b5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "ef2ed027e80b49ad82dfebe139ff10ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3374ce91c1df4014b48e388a454a5e5d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ff234810a9d346699ee6151dfb9d87f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "df2e59b88f274160a8ffeb851b9a2b92": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_778028e834a34d3dbe88e7bdfe81c95c",
              "IPY_MODEL_0e0612538d0e4caca9a4bd28845f2565",
              "IPY_MODEL_377336d719d345f1923a4135a37dc272"
            ],
            "layout": "IPY_MODEL_f185c98ee7c24414b7c7a6652b503310"
          }
        },
        "778028e834a34d3dbe88e7bdfe81c95c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3557dd72020746e0854cfe6d29b67b83",
            "placeholder": "​",
            "style": "IPY_MODEL_5f7041406ab3418d98f82ee38466b8b3",
            "value": "Generating splits...: 100%"
          }
        },
        "0e0612538d0e4caca9a4bd28845f2565": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c76917f142464807a2adaed63ff20e4b",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8b50cd58278b424d8cb525fe949370ce",
            "value": 1
          }
        },
        "377336d719d345f1923a4135a37dc272": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bed14bb01b8246db9e39676ce60d7ac2",
            "placeholder": "​",
            "style": "IPY_MODEL_ca0c78fea6e04b2bb9bf433ae879d7d3",
            "value": " 1/1 [01:17&lt;00:00, 77.02s/ splits]"
          }
        },
        "f185c98ee7c24414b7c7a6652b503310": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": "hidden",
            "width": null
          }
        },
        "3557dd72020746e0854cfe6d29b67b83": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5f7041406ab3418d98f82ee38466b8b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c76917f142464807a2adaed63ff20e4b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8b50cd58278b424d8cb525fe949370ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "bed14bb01b8246db9e39676ce60d7ac2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ca0c78fea6e04b2bb9bf433ae879d7d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "40128c7526ba4d34a6861ac36de9740f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3eba491f7e6a491684b4a24baf1cc25a",
              "IPY_MODEL_e0a69de1e5b04f94875742bc62d3766d",
              "IPY_MODEL_66a0f9d989904c91975f4b401fab0497"
            ],
            "layout": "IPY_MODEL_01efece8222e4c308f24ae9269ba0f72"
          }
        },
        "3eba491f7e6a491684b4a24baf1cc25a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6d982effb4594087a607518a0b6604fa",
            "placeholder": "​",
            "style": "IPY_MODEL_23e33cd88d964b03b71c0d8ef3b24891",
            "value": "Generating train examples...:  99%"
          }
        },
        "e0a69de1e5b04f94875742bc62d3766d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3bd9367ed59548cdb2ebfcc36878adab",
            "max": 23262,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7a4254a5dc3947d7aede0677213deb75",
            "value": 23262
          }
        },
        "66a0f9d989904c91975f4b401fab0497": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bb5258c2d86144bdb162bc5f1e5d6b4e",
            "placeholder": "​",
            "style": "IPY_MODEL_c183345dba344e17ad0d82957b973283",
            "value": " 23022/23262 [01:11&lt;00:01, 164.58 examples/s]"
          }
        },
        "01efece8222e4c308f24ae9269ba0f72": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": "hidden",
            "width": null
          }
        },
        "6d982effb4594087a607518a0b6604fa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "23e33cd88d964b03b71c0d8ef3b24891": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3bd9367ed59548cdb2ebfcc36878adab": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7a4254a5dc3947d7aede0677213deb75": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "bb5258c2d86144bdb162bc5f1e5d6b4e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c183345dba344e17ad0d82957b973283": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "09d1b6dce6dd4360b9bb2e8a1bed7abd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_610f1b4a7e464df0bead1cc67790d2c4",
              "IPY_MODEL_2344fc2e4e3f4c769c0bf5612fd5c7e0",
              "IPY_MODEL_d430829769384484a9fe6ddb4fc6ee8e"
            ],
            "layout": "IPY_MODEL_b816d93e95ea48219d7edf10ef9acba9"
          }
        },
        "610f1b4a7e464df0bead1cc67790d2c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4cacfb33f95b469f8028435d0c23fc6e",
            "placeholder": "​",
            "style": "IPY_MODEL_763cf6adaaff4823a0ab85791947a57c",
            "value": "Shuffling /root/tensorflow_datasets/cats_vs_dogs/4.0.1.incompleteQG49II/cats_vs_dogs-train.tfrecord*...:  97%"
          }
        },
        "2344fc2e4e3f4c769c0bf5612fd5c7e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cafac0222282470f9badf127913c8f53",
            "max": 23262,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7c5bb0964f1341078644bcf27981c0e4",
            "value": 23262
          }
        },
        "d430829769384484a9fe6ddb4fc6ee8e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dfd63b99b6654e938a9a6cd2425db806",
            "placeholder": "​",
            "style": "IPY_MODEL_531ca2fd75e34db994dde3b308e2e103",
            "value": " 22658/23262 [00:03&lt;00:00, 4614.33 examples/s]"
          }
        },
        "b816d93e95ea48219d7edf10ef9acba9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": "hidden",
            "width": null
          }
        },
        "4cacfb33f95b469f8028435d0c23fc6e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "763cf6adaaff4823a0ab85791947a57c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cafac0222282470f9badf127913c8f53": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7c5bb0964f1341078644bcf27981c0e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "dfd63b99b6654e938a9a6cd2425db806": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "531ca2fd75e34db994dde3b308e2e103": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "hP3Mitbi1od0"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from tensorflow.keras.applications.mobilenet import preprocess_input\n",
        "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, GlobalAveragePooling2D\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import tensorflow.keras.applications\n",
        "for model in dir(tensorflow.keras.applications):\n",
        "  print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "27V2xd0S2TGA",
        "outputId": "5cdda79d-9cf4-4724-999f-6ab50a460d01"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ConvNeXtBase\n",
            "ConvNeXtLarge\n",
            "ConvNeXtSmall\n",
            "ConvNeXtTiny\n",
            "ConvNeXtXLarge\n",
            "DenseNet121\n",
            "DenseNet169\n",
            "DenseNet201\n",
            "EfficientNetB0\n",
            "EfficientNetB1\n",
            "EfficientNetB2\n",
            "EfficientNetB3\n",
            "EfficientNetB4\n",
            "EfficientNetB5\n",
            "EfficientNetB6\n",
            "EfficientNetB7\n",
            "EfficientNetV2B0\n",
            "EfficientNetV2B1\n",
            "EfficientNetV2B2\n",
            "EfficientNetV2B3\n",
            "EfficientNetV2L\n",
            "EfficientNetV2M\n",
            "EfficientNetV2S\n",
            "InceptionResNetV2\n",
            "InceptionV3\n",
            "MobileNet\n",
            "MobileNetV2\n",
            "MobileNetV3Large\n",
            "MobileNetV3Small\n",
            "NASNetLarge\n",
            "NASNetMobile\n",
            "RegNetX002\n",
            "RegNetX004\n",
            "RegNetX006\n",
            "RegNetX008\n",
            "RegNetX016\n",
            "RegNetX032\n",
            "RegNetX040\n",
            "RegNetX064\n",
            "RegNetX080\n",
            "RegNetX120\n",
            "RegNetX160\n",
            "RegNetX320\n",
            "RegNetY002\n",
            "RegNetY004\n",
            "RegNetY006\n",
            "RegNetY008\n",
            "RegNetY016\n",
            "RegNetY032\n",
            "RegNetY040\n",
            "RegNetY064\n",
            "RegNetY080\n",
            "RegNetY120\n",
            "RegNetY160\n",
            "RegNetY320\n",
            "ResNet101\n",
            "ResNet101V2\n",
            "ResNet152\n",
            "ResNet152V2\n",
            "ResNet50\n",
            "ResNet50V2\n",
            "ResNetRS101\n",
            "ResNetRS152\n",
            "ResNetRS200\n",
            "ResNetRS270\n",
            "ResNetRS350\n",
            "ResNetRS420\n",
            "ResNetRS50\n",
            "VGG16\n",
            "VGG19\n",
            "Xception\n",
            "__builtins__\n",
            "__cached__\n",
            "__doc__\n",
            "__file__\n",
            "__loader__\n",
            "__name__\n",
            "__package__\n",
            "__path__\n",
            "__spec__\n",
            "convnext\n",
            "densenet\n",
            "efficientnet\n",
            "efficientnet_v2\n",
            "imagenet_utils\n",
            "inception_resnet_v2\n",
            "inception_v3\n",
            "mobilenet\n",
            "mobilenet_v2\n",
            "mobilenet_v3\n",
            "nasnet\n",
            "regnet\n",
            "resnet\n",
            "resnet50\n",
            "resnet_rs\n",
            "resnet_v2\n",
            "vgg16\n",
            "vgg19\n",
            "xception\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "help(tensorflow.keras.applications)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FsqrhwS55ChF",
        "outputId": "fc4fe43b-427b-4834-d874-e60f179974d7"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Help on package tensorflow.keras.applications in tensorflow.keras:\n",
            "\n",
            "NAME\n",
            "    tensorflow.keras.applications - AUTOGENERATED. DO NOT EDIT.\n",
            "\n",
            "PACKAGE CONTENTS\n",
            "    convnext (package)\n",
            "    densenet (package)\n",
            "    efficientnet (package)\n",
            "    efficientnet_v2 (package)\n",
            "    imagenet_utils (package)\n",
            "    inception_resnet_v2 (package)\n",
            "    inception_v3 (package)\n",
            "    mobilenet (package)\n",
            "    mobilenet_v2 (package)\n",
            "    mobilenet_v3 (package)\n",
            "    nasnet (package)\n",
            "    regnet (package)\n",
            "    resnet (package)\n",
            "    resnet50 (package)\n",
            "    resnet_rs (package)\n",
            "    resnet_v2 (package)\n",
            "    vgg16 (package)\n",
            "    vgg19 (package)\n",
            "    xception (package)\n",
            "\n",
            "FILE\n",
            "    /usr/local/lib/python3.10/dist-packages/keras/api/_v2/keras/applications/__init__.py\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#include_top helps decide whether to include fully connected layer of the model, we do not want to include it so defined as False\n",
        "base_model = MobileNetV2(weights='imagenet', include_top =False)\n",
        "base_model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jnELdZLX5asS",
        "outputId": "ec71bb8d-600b-429f-dbcf-7be34a0b099c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5\n",
            "9406464/9406464 [==============================] - 0s 0us/step\n",
            "Model: \"mobilenetv2_1.00_224\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)        [(None, None, None, 3)]      0         []                            \n",
            "                                                                                                  \n",
            " Conv1 (Conv2D)              (None, None, None, 32)       864       ['input_1[0][0]']             \n",
            "                                                                                                  \n",
            " bn_Conv1 (BatchNormalizati  (None, None, None, 32)       128       ['Conv1[0][0]']               \n",
            " on)                                                                                              \n",
            "                                                                                                  \n",
            " Conv1_relu (ReLU)           (None, None, None, 32)       0         ['bn_Conv1[0][0]']            \n",
            "                                                                                                  \n",
            " expanded_conv_depthwise (D  (None, None, None, 32)       288       ['Conv1_relu[0][0]']          \n",
            " epthwiseConv2D)                                                                                  \n",
            "                                                                                                  \n",
            " expanded_conv_depthwise_BN  (None, None, None, 32)       128       ['expanded_conv_depthwise[0][0\n",
            "  (BatchNormalization)                                              ]']                           \n",
            "                                                                                                  \n",
            " expanded_conv_depthwise_re  (None, None, None, 32)       0         ['expanded_conv_depthwise_BN[0\n",
            " lu (ReLU)                                                          ][0]']                        \n",
            "                                                                                                  \n",
            " expanded_conv_project (Con  (None, None, None, 16)       512       ['expanded_conv_depthwise_relu\n",
            " v2D)                                                               [0][0]']                      \n",
            "                                                                                                  \n",
            " expanded_conv_project_BN (  (None, None, None, 16)       64        ['expanded_conv_project[0][0]'\n",
            " BatchNormalization)                                                ]                             \n",
            "                                                                                                  \n",
            " block_1_expand (Conv2D)     (None, None, None, 96)       1536      ['expanded_conv_project_BN[0][\n",
            "                                                                    0]']                          \n",
            "                                                                                                  \n",
            " block_1_expand_BN (BatchNo  (None, None, None, 96)       384       ['block_1_expand[0][0]']      \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " block_1_expand_relu (ReLU)  (None, None, None, 96)       0         ['block_1_expand_BN[0][0]']   \n",
            "                                                                                                  \n",
            " block_1_pad (ZeroPadding2D  (None, None, None, 96)       0         ['block_1_expand_relu[0][0]'] \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " block_1_depthwise (Depthwi  (None, None, None, 96)       864       ['block_1_pad[0][0]']         \n",
            " seConv2D)                                                                                        \n",
            "                                                                                                  \n",
            " block_1_depthwise_BN (Batc  (None, None, None, 96)       384       ['block_1_depthwise[0][0]']   \n",
            " hNormalization)                                                                                  \n",
            "                                                                                                  \n",
            " block_1_depthwise_relu (Re  (None, None, None, 96)       0         ['block_1_depthwise_BN[0][0]']\n",
            " LU)                                                                                              \n",
            "                                                                                                  \n",
            " block_1_project (Conv2D)    (None, None, None, 24)       2304      ['block_1_depthwise_relu[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " block_1_project_BN (BatchN  (None, None, None, 24)       96        ['block_1_project[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " block_2_expand (Conv2D)     (None, None, None, 144)      3456      ['block_1_project_BN[0][0]']  \n",
            "                                                                                                  \n",
            " block_2_expand_BN (BatchNo  (None, None, None, 144)      576       ['block_2_expand[0][0]']      \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " block_2_expand_relu (ReLU)  (None, None, None, 144)      0         ['block_2_expand_BN[0][0]']   \n",
            "                                                                                                  \n",
            " block_2_depthwise (Depthwi  (None, None, None, 144)      1296      ['block_2_expand_relu[0][0]'] \n",
            " seConv2D)                                                                                        \n",
            "                                                                                                  \n",
            " block_2_depthwise_BN (Batc  (None, None, None, 144)      576       ['block_2_depthwise[0][0]']   \n",
            " hNormalization)                                                                                  \n",
            "                                                                                                  \n",
            " block_2_depthwise_relu (Re  (None, None, None, 144)      0         ['block_2_depthwise_BN[0][0]']\n",
            " LU)                                                                                              \n",
            "                                                                                                  \n",
            " block_2_project (Conv2D)    (None, None, None, 24)       3456      ['block_2_depthwise_relu[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " block_2_project_BN (BatchN  (None, None, None, 24)       96        ['block_2_project[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " block_2_add (Add)           (None, None, None, 24)       0         ['block_1_project_BN[0][0]',  \n",
            "                                                                     'block_2_project_BN[0][0]']  \n",
            "                                                                                                  \n",
            " block_3_expand (Conv2D)     (None, None, None, 144)      3456      ['block_2_add[0][0]']         \n",
            "                                                                                                  \n",
            " block_3_expand_BN (BatchNo  (None, None, None, 144)      576       ['block_3_expand[0][0]']      \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " block_3_expand_relu (ReLU)  (None, None, None, 144)      0         ['block_3_expand_BN[0][0]']   \n",
            "                                                                                                  \n",
            " block_3_pad (ZeroPadding2D  (None, None, None, 144)      0         ['block_3_expand_relu[0][0]'] \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " block_3_depthwise (Depthwi  (None, None, None, 144)      1296      ['block_3_pad[0][0]']         \n",
            " seConv2D)                                                                                        \n",
            "                                                                                                  \n",
            " block_3_depthwise_BN (Batc  (None, None, None, 144)      576       ['block_3_depthwise[0][0]']   \n",
            " hNormalization)                                                                                  \n",
            "                                                                                                  \n",
            " block_3_depthwise_relu (Re  (None, None, None, 144)      0         ['block_3_depthwise_BN[0][0]']\n",
            " LU)                                                                                              \n",
            "                                                                                                  \n",
            " block_3_project (Conv2D)    (None, None, None, 32)       4608      ['block_3_depthwise_relu[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " block_3_project_BN (BatchN  (None, None, None, 32)       128       ['block_3_project[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " block_4_expand (Conv2D)     (None, None, None, 192)      6144      ['block_3_project_BN[0][0]']  \n",
            "                                                                                                  \n",
            " block_4_expand_BN (BatchNo  (None, None, None, 192)      768       ['block_4_expand[0][0]']      \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " block_4_expand_relu (ReLU)  (None, None, None, 192)      0         ['block_4_expand_BN[0][0]']   \n",
            "                                                                                                  \n",
            " block_4_depthwise (Depthwi  (None, None, None, 192)      1728      ['block_4_expand_relu[0][0]'] \n",
            " seConv2D)                                                                                        \n",
            "                                                                                                  \n",
            " block_4_depthwise_BN (Batc  (None, None, None, 192)      768       ['block_4_depthwise[0][0]']   \n",
            " hNormalization)                                                                                  \n",
            "                                                                                                  \n",
            " block_4_depthwise_relu (Re  (None, None, None, 192)      0         ['block_4_depthwise_BN[0][0]']\n",
            " LU)                                                                                              \n",
            "                                                                                                  \n",
            " block_4_project (Conv2D)    (None, None, None, 32)       6144      ['block_4_depthwise_relu[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " block_4_project_BN (BatchN  (None, None, None, 32)       128       ['block_4_project[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " block_4_add (Add)           (None, None, None, 32)       0         ['block_3_project_BN[0][0]',  \n",
            "                                                                     'block_4_project_BN[0][0]']  \n",
            "                                                                                                  \n",
            " block_5_expand (Conv2D)     (None, None, None, 192)      6144      ['block_4_add[0][0]']         \n",
            "                                                                                                  \n",
            " block_5_expand_BN (BatchNo  (None, None, None, 192)      768       ['block_5_expand[0][0]']      \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " block_5_expand_relu (ReLU)  (None, None, None, 192)      0         ['block_5_expand_BN[0][0]']   \n",
            "                                                                                                  \n",
            " block_5_depthwise (Depthwi  (None, None, None, 192)      1728      ['block_5_expand_relu[0][0]'] \n",
            " seConv2D)                                                                                        \n",
            "                                                                                                  \n",
            " block_5_depthwise_BN (Batc  (None, None, None, 192)      768       ['block_5_depthwise[0][0]']   \n",
            " hNormalization)                                                                                  \n",
            "                                                                                                  \n",
            " block_5_depthwise_relu (Re  (None, None, None, 192)      0         ['block_5_depthwise_BN[0][0]']\n",
            " LU)                                                                                              \n",
            "                                                                                                  \n",
            " block_5_project (Conv2D)    (None, None, None, 32)       6144      ['block_5_depthwise_relu[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " block_5_project_BN (BatchN  (None, None, None, 32)       128       ['block_5_project[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " block_5_add (Add)           (None, None, None, 32)       0         ['block_4_add[0][0]',         \n",
            "                                                                     'block_5_project_BN[0][0]']  \n",
            "                                                                                                  \n",
            " block_6_expand (Conv2D)     (None, None, None, 192)      6144      ['block_5_add[0][0]']         \n",
            "                                                                                                  \n",
            " block_6_expand_BN (BatchNo  (None, None, None, 192)      768       ['block_6_expand[0][0]']      \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " block_6_expand_relu (ReLU)  (None, None, None, 192)      0         ['block_6_expand_BN[0][0]']   \n",
            "                                                                                                  \n",
            " block_6_pad (ZeroPadding2D  (None, None, None, 192)      0         ['block_6_expand_relu[0][0]'] \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " block_6_depthwise (Depthwi  (None, None, None, 192)      1728      ['block_6_pad[0][0]']         \n",
            " seConv2D)                                                                                        \n",
            "                                                                                                  \n",
            " block_6_depthwise_BN (Batc  (None, None, None, 192)      768       ['block_6_depthwise[0][0]']   \n",
            " hNormalization)                                                                                  \n",
            "                                                                                                  \n",
            " block_6_depthwise_relu (Re  (None, None, None, 192)      0         ['block_6_depthwise_BN[0][0]']\n",
            " LU)                                                                                              \n",
            "                                                                                                  \n",
            " block_6_project (Conv2D)    (None, None, None, 64)       12288     ['block_6_depthwise_relu[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " block_6_project_BN (BatchN  (None, None, None, 64)       256       ['block_6_project[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " block_7_expand (Conv2D)     (None, None, None, 384)      24576     ['block_6_project_BN[0][0]']  \n",
            "                                                                                                  \n",
            " block_7_expand_BN (BatchNo  (None, None, None, 384)      1536      ['block_7_expand[0][0]']      \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " block_7_expand_relu (ReLU)  (None, None, None, 384)      0         ['block_7_expand_BN[0][0]']   \n",
            "                                                                                                  \n",
            " block_7_depthwise (Depthwi  (None, None, None, 384)      3456      ['block_7_expand_relu[0][0]'] \n",
            " seConv2D)                                                                                        \n",
            "                                                                                                  \n",
            " block_7_depthwise_BN (Batc  (None, None, None, 384)      1536      ['block_7_depthwise[0][0]']   \n",
            " hNormalization)                                                                                  \n",
            "                                                                                                  \n",
            " block_7_depthwise_relu (Re  (None, None, None, 384)      0         ['block_7_depthwise_BN[0][0]']\n",
            " LU)                                                                                              \n",
            "                                                                                                  \n",
            " block_7_project (Conv2D)    (None, None, None, 64)       24576     ['block_7_depthwise_relu[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " block_7_project_BN (BatchN  (None, None, None, 64)       256       ['block_7_project[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " block_7_add (Add)           (None, None, None, 64)       0         ['block_6_project_BN[0][0]',  \n",
            "                                                                     'block_7_project_BN[0][0]']  \n",
            "                                                                                                  \n",
            " block_8_expand (Conv2D)     (None, None, None, 384)      24576     ['block_7_add[0][0]']         \n",
            "                                                                                                  \n",
            " block_8_expand_BN (BatchNo  (None, None, None, 384)      1536      ['block_8_expand[0][0]']      \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " block_8_expand_relu (ReLU)  (None, None, None, 384)      0         ['block_8_expand_BN[0][0]']   \n",
            "                                                                                                  \n",
            " block_8_depthwise (Depthwi  (None, None, None, 384)      3456      ['block_8_expand_relu[0][0]'] \n",
            " seConv2D)                                                                                        \n",
            "                                                                                                  \n",
            " block_8_depthwise_BN (Batc  (None, None, None, 384)      1536      ['block_8_depthwise[0][0]']   \n",
            " hNormalization)                                                                                  \n",
            "                                                                                                  \n",
            " block_8_depthwise_relu (Re  (None, None, None, 384)      0         ['block_8_depthwise_BN[0][0]']\n",
            " LU)                                                                                              \n",
            "                                                                                                  \n",
            " block_8_project (Conv2D)    (None, None, None, 64)       24576     ['block_8_depthwise_relu[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " block_8_project_BN (BatchN  (None, None, None, 64)       256       ['block_8_project[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " block_8_add (Add)           (None, None, None, 64)       0         ['block_7_add[0][0]',         \n",
            "                                                                     'block_8_project_BN[0][0]']  \n",
            "                                                                                                  \n",
            " block_9_expand (Conv2D)     (None, None, None, 384)      24576     ['block_8_add[0][0]']         \n",
            "                                                                                                  \n",
            " block_9_expand_BN (BatchNo  (None, None, None, 384)      1536      ['block_9_expand[0][0]']      \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " block_9_expand_relu (ReLU)  (None, None, None, 384)      0         ['block_9_expand_BN[0][0]']   \n",
            "                                                                                                  \n",
            " block_9_depthwise (Depthwi  (None, None, None, 384)      3456      ['block_9_expand_relu[0][0]'] \n",
            " seConv2D)                                                                                        \n",
            "                                                                                                  \n",
            " block_9_depthwise_BN (Batc  (None, None, None, 384)      1536      ['block_9_depthwise[0][0]']   \n",
            " hNormalization)                                                                                  \n",
            "                                                                                                  \n",
            " block_9_depthwise_relu (Re  (None, None, None, 384)      0         ['block_9_depthwise_BN[0][0]']\n",
            " LU)                                                                                              \n",
            "                                                                                                  \n",
            " block_9_project (Conv2D)    (None, None, None, 64)       24576     ['block_9_depthwise_relu[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " block_9_project_BN (BatchN  (None, None, None, 64)       256       ['block_9_project[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " block_9_add (Add)           (None, None, None, 64)       0         ['block_8_add[0][0]',         \n",
            "                                                                     'block_9_project_BN[0][0]']  \n",
            "                                                                                                  \n",
            " block_10_expand (Conv2D)    (None, None, None, 384)      24576     ['block_9_add[0][0]']         \n",
            "                                                                                                  \n",
            " block_10_expand_BN (BatchN  (None, None, None, 384)      1536      ['block_10_expand[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " block_10_expand_relu (ReLU  (None, None, None, 384)      0         ['block_10_expand_BN[0][0]']  \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " block_10_depthwise (Depthw  (None, None, None, 384)      3456      ['block_10_expand_relu[0][0]']\n",
            " iseConv2D)                                                                                       \n",
            "                                                                                                  \n",
            " block_10_depthwise_BN (Bat  (None, None, None, 384)      1536      ['block_10_depthwise[0][0]']  \n",
            " chNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " block_10_depthwise_relu (R  (None, None, None, 384)      0         ['block_10_depthwise_BN[0][0]'\n",
            " eLU)                                                               ]                             \n",
            "                                                                                                  \n",
            " block_10_project (Conv2D)   (None, None, None, 96)       36864     ['block_10_depthwise_relu[0][0\n",
            "                                                                    ]']                           \n",
            "                                                                                                  \n",
            " block_10_project_BN (Batch  (None, None, None, 96)       384       ['block_10_project[0][0]']    \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " block_11_expand (Conv2D)    (None, None, None, 576)      55296     ['block_10_project_BN[0][0]'] \n",
            "                                                                                                  \n",
            " block_11_expand_BN (BatchN  (None, None, None, 576)      2304      ['block_11_expand[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " block_11_expand_relu (ReLU  (None, None, None, 576)      0         ['block_11_expand_BN[0][0]']  \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " block_11_depthwise (Depthw  (None, None, None, 576)      5184      ['block_11_expand_relu[0][0]']\n",
            " iseConv2D)                                                                                       \n",
            "                                                                                                  \n",
            " block_11_depthwise_BN (Bat  (None, None, None, 576)      2304      ['block_11_depthwise[0][0]']  \n",
            " chNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " block_11_depthwise_relu (R  (None, None, None, 576)      0         ['block_11_depthwise_BN[0][0]'\n",
            " eLU)                                                               ]                             \n",
            "                                                                                                  \n",
            " block_11_project (Conv2D)   (None, None, None, 96)       55296     ['block_11_depthwise_relu[0][0\n",
            "                                                                    ]']                           \n",
            "                                                                                                  \n",
            " block_11_project_BN (Batch  (None, None, None, 96)       384       ['block_11_project[0][0]']    \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " block_11_add (Add)          (None, None, None, 96)       0         ['block_10_project_BN[0][0]', \n",
            "                                                                     'block_11_project_BN[0][0]'] \n",
            "                                                                                                  \n",
            " block_12_expand (Conv2D)    (None, None, None, 576)      55296     ['block_11_add[0][0]']        \n",
            "                                                                                                  \n",
            " block_12_expand_BN (BatchN  (None, None, None, 576)      2304      ['block_12_expand[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " block_12_expand_relu (ReLU  (None, None, None, 576)      0         ['block_12_expand_BN[0][0]']  \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " block_12_depthwise (Depthw  (None, None, None, 576)      5184      ['block_12_expand_relu[0][0]']\n",
            " iseConv2D)                                                                                       \n",
            "                                                                                                  \n",
            " block_12_depthwise_BN (Bat  (None, None, None, 576)      2304      ['block_12_depthwise[0][0]']  \n",
            " chNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " block_12_depthwise_relu (R  (None, None, None, 576)      0         ['block_12_depthwise_BN[0][0]'\n",
            " eLU)                                                               ]                             \n",
            "                                                                                                  \n",
            " block_12_project (Conv2D)   (None, None, None, 96)       55296     ['block_12_depthwise_relu[0][0\n",
            "                                                                    ]']                           \n",
            "                                                                                                  \n",
            " block_12_project_BN (Batch  (None, None, None, 96)       384       ['block_12_project[0][0]']    \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " block_12_add (Add)          (None, None, None, 96)       0         ['block_11_add[0][0]',        \n",
            "                                                                     'block_12_project_BN[0][0]'] \n",
            "                                                                                                  \n",
            " block_13_expand (Conv2D)    (None, None, None, 576)      55296     ['block_12_add[0][0]']        \n",
            "                                                                                                  \n",
            " block_13_expand_BN (BatchN  (None, None, None, 576)      2304      ['block_13_expand[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " block_13_expand_relu (ReLU  (None, None, None, 576)      0         ['block_13_expand_BN[0][0]']  \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " block_13_pad (ZeroPadding2  (None, None, None, 576)      0         ['block_13_expand_relu[0][0]']\n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " block_13_depthwise (Depthw  (None, None, None, 576)      5184      ['block_13_pad[0][0]']        \n",
            " iseConv2D)                                                                                       \n",
            "                                                                                                  \n",
            " block_13_depthwise_BN (Bat  (None, None, None, 576)      2304      ['block_13_depthwise[0][0]']  \n",
            " chNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " block_13_depthwise_relu (R  (None, None, None, 576)      0         ['block_13_depthwise_BN[0][0]'\n",
            " eLU)                                                               ]                             \n",
            "                                                                                                  \n",
            " block_13_project (Conv2D)   (None, None, None, 160)      92160     ['block_13_depthwise_relu[0][0\n",
            "                                                                    ]']                           \n",
            "                                                                                                  \n",
            " block_13_project_BN (Batch  (None, None, None, 160)      640       ['block_13_project[0][0]']    \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " block_14_expand (Conv2D)    (None, None, None, 960)      153600    ['block_13_project_BN[0][0]'] \n",
            "                                                                                                  \n",
            " block_14_expand_BN (BatchN  (None, None, None, 960)      3840      ['block_14_expand[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " block_14_expand_relu (ReLU  (None, None, None, 960)      0         ['block_14_expand_BN[0][0]']  \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " block_14_depthwise (Depthw  (None, None, None, 960)      8640      ['block_14_expand_relu[0][0]']\n",
            " iseConv2D)                                                                                       \n",
            "                                                                                                  \n",
            " block_14_depthwise_BN (Bat  (None, None, None, 960)      3840      ['block_14_depthwise[0][0]']  \n",
            " chNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " block_14_depthwise_relu (R  (None, None, None, 960)      0         ['block_14_depthwise_BN[0][0]'\n",
            " eLU)                                                               ]                             \n",
            "                                                                                                  \n",
            " block_14_project (Conv2D)   (None, None, None, 160)      153600    ['block_14_depthwise_relu[0][0\n",
            "                                                                    ]']                           \n",
            "                                                                                                  \n",
            " block_14_project_BN (Batch  (None, None, None, 160)      640       ['block_14_project[0][0]']    \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " block_14_add (Add)          (None, None, None, 160)      0         ['block_13_project_BN[0][0]', \n",
            "                                                                     'block_14_project_BN[0][0]'] \n",
            "                                                                                                  \n",
            " block_15_expand (Conv2D)    (None, None, None, 960)      153600    ['block_14_add[0][0]']        \n",
            "                                                                                                  \n",
            " block_15_expand_BN (BatchN  (None, None, None, 960)      3840      ['block_15_expand[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " block_15_expand_relu (ReLU  (None, None, None, 960)      0         ['block_15_expand_BN[0][0]']  \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " block_15_depthwise (Depthw  (None, None, None, 960)      8640      ['block_15_expand_relu[0][0]']\n",
            " iseConv2D)                                                                                       \n",
            "                                                                                                  \n",
            " block_15_depthwise_BN (Bat  (None, None, None, 960)      3840      ['block_15_depthwise[0][0]']  \n",
            " chNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " block_15_depthwise_relu (R  (None, None, None, 960)      0         ['block_15_depthwise_BN[0][0]'\n",
            " eLU)                                                               ]                             \n",
            "                                                                                                  \n",
            " block_15_project (Conv2D)   (None, None, None, 160)      153600    ['block_15_depthwise_relu[0][0\n",
            "                                                                    ]']                           \n",
            "                                                                                                  \n",
            " block_15_project_BN (Batch  (None, None, None, 160)      640       ['block_15_project[0][0]']    \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " block_15_add (Add)          (None, None, None, 160)      0         ['block_14_add[0][0]',        \n",
            "                                                                     'block_15_project_BN[0][0]'] \n",
            "                                                                                                  \n",
            " block_16_expand (Conv2D)    (None, None, None, 960)      153600    ['block_15_add[0][0]']        \n",
            "                                                                                                  \n",
            " block_16_expand_BN (BatchN  (None, None, None, 960)      3840      ['block_16_expand[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " block_16_expand_relu (ReLU  (None, None, None, 960)      0         ['block_16_expand_BN[0][0]']  \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " block_16_depthwise (Depthw  (None, None, None, 960)      8640      ['block_16_expand_relu[0][0]']\n",
            " iseConv2D)                                                                                       \n",
            "                                                                                                  \n",
            " block_16_depthwise_BN (Bat  (None, None, None, 960)      3840      ['block_16_depthwise[0][0]']  \n",
            " chNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " block_16_depthwise_relu (R  (None, None, None, 960)      0         ['block_16_depthwise_BN[0][0]'\n",
            " eLU)                                                               ]                             \n",
            "                                                                                                  \n",
            " block_16_project (Conv2D)   (None, None, None, 320)      307200    ['block_16_depthwise_relu[0][0\n",
            "                                                                    ]']                           \n",
            "                                                                                                  \n",
            " block_16_project_BN (Batch  (None, None, None, 320)      1280      ['block_16_project[0][0]']    \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " Conv_1 (Conv2D)             (None, None, None, 1280)     409600    ['block_16_project_BN[0][0]'] \n",
            "                                                                                                  \n",
            " Conv_1_bn (BatchNormalizat  (None, None, None, 1280)     5120      ['Conv_1[0][0]']              \n",
            " ion)                                                                                             \n",
            "                                                                                                  \n",
            " out_relu (ReLU)             (None, None, None, 1280)     0         ['Conv_1_bn[0][0]']           \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 2257984 (8.61 MB)\n",
            "Trainable params: 2223872 (8.48 MB)\n",
            "Non-trainable params: 34112 (133.25 KB)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_shape = (299, 299, 3)\n",
        "\n",
        "input_tensor = tf.keras.Input(shape=input_shape)\n",
        "rotate = tf.keras.layers.RandomRotation(0.4)(input_tensor)\n",
        "width = tf.keras.layers.RandomWidth(0.2)(rotate)\n",
        "height = tf.keras.layers.RandomHeight(0.2)(width)\n",
        "zoom = tf.keras.layers.RandomZoom(0.2)(height)\n",
        "flip = tf.keras.layers.RandomFlip('horizontal_and_vertical')(zoom)\n",
        "baseoutput = base_model(flip)\n",
        "globavg = GlobalAveragePooling2D()(baseoutput)\n",
        "dropout1 = Dropout(0.4)(globavg)\n",
        "bn1 = BatchNormalization()(dropout1)\n",
        "dense1 = Dense(256, activation='relu')(bn1)\n",
        "dropout2 = Dropout(0.4)(dense1)\n",
        "predictions = Dense(1, activation='sigmoid')(dropout2)\n",
        "\n",
        "model = Model(inputs=input_tensor, outputs=predictions)\n",
        "\n",
        "for layer in base_model.layers:\n",
        "  layer.trainable=False\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])"
      ],
      "metadata": {
        "id": "kro34m207mjo"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BKg-i9m5CoFh",
        "outputId": "9f01fea8-8c27-4e03-9f76-e0f6e6c8b97a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_2 (InputLayer)        [(None, 299, 299, 3)]     0         \n",
            "                                                                 \n",
            " random_rotation (RandomRot  (None, 299, 299, 3)       0         \n",
            " ation)                                                          \n",
            "                                                                 \n",
            " random_width (RandomWidth)  (None, 299, None, 3)      0         \n",
            "                                                                 \n",
            " random_height (RandomHeigh  (None, None, None, 3)     0         \n",
            " t)                                                              \n",
            "                                                                 \n",
            " random_zoom (RandomZoom)    (None, None, None, 3)     0         \n",
            "                                                                 \n",
            " random_flip (RandomFlip)    (None, None, None, 3)     0         \n",
            "                                                                 \n",
            " mobilenetv2_1.00_224 (Func  (None, None, None, 1280   2257984   \n",
            " tional)                     )                                   \n",
            "                                                                 \n",
            " global_average_pooling2d (  (None, 1280)              0         \n",
            " GlobalAveragePooling2D)                                         \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 1280)              0         \n",
            "                                                                 \n",
            " batch_normalization (Batch  (None, 1280)              5120      \n",
            " Normalization)                                                  \n",
            "                                                                 \n",
            " dense (Dense)               (None, 256)               327936    \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 256)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 257       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2591297 (9.89 MB)\n",
            "Trainable params: 330753 (1.26 MB)\n",
            "Non-trainable params: 2260544 (8.62 MB)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "split = ['train[:80%]', 'train[20%:]']\n",
        "(train_dataset, val_dataset), info = tfds.load('cats_vs_dogs', split=split, with_info=True, as_supervised=True)"
      ],
      "metadata": {
        "id": "-ogg95qbFhmk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136,
          "referenced_widgets": [
            "9d968df4de6b44ed8fd5895a0cc73bec",
            "c8beba62315a48bc85e4b72cf481c5a7",
            "9dff21869d2f4dde851d59a1b8c6d4b0",
            "f487bee11a34450e899df6ff4b0ae0cd",
            "9a266261140a4b028271de068da2c186",
            "c0e5a9ffa19e4a78ac7fd1a84e2f2809",
            "95b043ea17704d85974eee44a87dbd99",
            "cd4f12e9bc54438bb09a99e3c11be3c5",
            "5a27e230024e48ecaa1416ec5edf6ea3",
            "f039ef6526da41e8b0917487b38b0f02",
            "2d5677f122d04591b3cf25f31060febf",
            "b59b71596d7b4ef1912e5b230e21b5a2",
            "430d5d35461640518cc67d14c43abd41",
            "9f0ee101b83a419489a32ba6396c412c",
            "5722a7a24071492e8db5abfcaa8467ae",
            "c3fc5afed0f2425393206f775ec435b1",
            "6e1661af9e05418b89ddee67f351ec57",
            "9216f972148044a6978efdc864784000",
            "f0c6af735b614e67a56169515b9416b5",
            "ef2ed027e80b49ad82dfebe139ff10ed",
            "3374ce91c1df4014b48e388a454a5e5d",
            "ff234810a9d346699ee6151dfb9d87f1",
            "df2e59b88f274160a8ffeb851b9a2b92",
            "778028e834a34d3dbe88e7bdfe81c95c",
            "0e0612538d0e4caca9a4bd28845f2565",
            "377336d719d345f1923a4135a37dc272",
            "f185c98ee7c24414b7c7a6652b503310",
            "3557dd72020746e0854cfe6d29b67b83",
            "5f7041406ab3418d98f82ee38466b8b3",
            "c76917f142464807a2adaed63ff20e4b",
            "8b50cd58278b424d8cb525fe949370ce",
            "bed14bb01b8246db9e39676ce60d7ac2",
            "ca0c78fea6e04b2bb9bf433ae879d7d3",
            "40128c7526ba4d34a6861ac36de9740f",
            "3eba491f7e6a491684b4a24baf1cc25a",
            "e0a69de1e5b04f94875742bc62d3766d",
            "66a0f9d989904c91975f4b401fab0497",
            "01efece8222e4c308f24ae9269ba0f72",
            "6d982effb4594087a607518a0b6604fa",
            "23e33cd88d964b03b71c0d8ef3b24891",
            "3bd9367ed59548cdb2ebfcc36878adab",
            "7a4254a5dc3947d7aede0677213deb75",
            "bb5258c2d86144bdb162bc5f1e5d6b4e",
            "c183345dba344e17ad0d82957b973283",
            "09d1b6dce6dd4360b9bb2e8a1bed7abd",
            "610f1b4a7e464df0bead1cc67790d2c4",
            "2344fc2e4e3f4c769c0bf5612fd5c7e0",
            "d430829769384484a9fe6ddb4fc6ee8e",
            "b816d93e95ea48219d7edf10ef9acba9",
            "4cacfb33f95b469f8028435d0c23fc6e",
            "763cf6adaaff4823a0ab85791947a57c",
            "cafac0222282470f9badf127913c8f53",
            "7c5bb0964f1341078644bcf27981c0e4",
            "dfd63b99b6654e938a9a6cd2425db806",
            "531ca2fd75e34db994dde3b308e2e103"
          ]
        },
        "outputId": "4873a380-d134-42ec-aef8-fdb7cdc36ab3"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading and preparing dataset 786.67 MiB (download: 786.67 MiB, generated: 1.04 GiB, total: 1.81 GiB) to /root/tensorflow_datasets/cats_vs_dogs/4.0.1...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Dl Completed...: 0 url [00:00, ? url/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9d968df4de6b44ed8fd5895a0cc73bec"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Dl Size...: 0 MiB [00:00, ? MiB/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b59b71596d7b4ef1912e5b230e21b5a2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating splits...:   0%|          | 0/1 [00:00<?, ? splits/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "df2e59b88f274160a8ffeb851b9a2b92"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train examples...:   0%|          | 0/23262 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "40128c7526ba4d34a6861ac36de9740f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:1738 images were corrupted and were skipped\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Shuffling /root/tensorflow_datasets/cats_vs_dogs/4.0.1.incompleteQG49II/cats_vs_dogs-train.tfrecord*...:   0%|…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "09d1b6dce6dd4360b9bb2e8a1bed7abd"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset cats_vs_dogs downloaded and prepared to /root/tensorflow_datasets/cats_vs_dogs/4.0.1. Subsequent calls will reuse this data.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "help(tfds)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uq3J-WKAv-lD",
        "outputId": "c13304ee-c9c4-4dcd-f417-1c43cd87dbe8"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Help on package tensorflow_datasets:\n",
            "\n",
            "NAME\n",
            "    tensorflow_datasets - `tensorflow_datasets` (`tfds`) defines a collection of datasets ready-to-use with TensorFlow.\n",
            "\n",
            "DESCRIPTION\n",
            "    Each dataset is defined as a `tfds.core.DatasetBuilder`, which encapsulates\n",
            "    the logic to download the dataset and construct an input pipeline, as well as\n",
            "    contains the dataset documentation (version, splits, number of examples, etc.).\n",
            "    \n",
            "    The main library entrypoints are:\n",
            "    \n",
            "    * `tfds.builder`: fetch a `tfds.core.DatasetBuilder` by name\n",
            "    * `tfds.load`: convenience method to construct a builder, download the data, and\n",
            "      create an input pipeline, returning a `tf.data.Dataset`.\n",
            "    \n",
            "    Documentation:\n",
            "    \n",
            "    * These API docs\n",
            "    * [Available datasets](https://www.tensorflow.org/datasets/catalog/overview)\n",
            "    * [Colab tutorial](https://colab.research.google.com/github/tensorflow/datasets/blob/master/docs/overview.ipynb)\n",
            "    * [Add a dataset](https://www.tensorflow.org/datasets/add_dataset)\n",
            "\n",
            "PACKAGE CONTENTS\n",
            "    audio (package)\n",
            "    conftest\n",
            "    core (package)\n",
            "    d4rl (package)\n",
            "    dataset_collections (package)\n",
            "    graphs (package)\n",
            "    image (package)\n",
            "    image_classification (package)\n",
            "    import_public_api_test\n",
            "    import_test\n",
            "    import_without_tf_test\n",
            "    nearest_neighbors (package)\n",
            "    object_detection (package)\n",
            "    proto (package)\n",
            "    public_api\n",
            "    question_answering (package)\n",
            "    ranking (package)\n",
            "    recommendation (package)\n",
            "    rl_unplugged (package)\n",
            "    rlds (package)\n",
            "    robomimic (package)\n",
            "    robotics (package)\n",
            "    scripts (package)\n",
            "    setup_teardown\n",
            "    structured (package)\n",
            "    summarization (package)\n",
            "    testing (package)\n",
            "    text (package)\n",
            "    text_simplification (package)\n",
            "    time_series (package)\n",
            "    translate (package)\n",
            "    typing\n",
            "    version\n",
            "    video (package)\n",
            "    vision_language (package)\n",
            "\n",
            "SUBMODULES\n",
            "    _call_metadata\n",
            "    _tfds_logging\n",
            "    beam\n",
            "    dataset_builders\n",
            "    datasets\n",
            "    decode\n",
            "    deprecated\n",
            "    download\n",
            "    features\n",
            "    folder_dataset\n",
            "    transform\n",
            "    visualization\n",
            "\n",
            "CLASSES\n",
            "    builtins.object\n",
            "        tensorflow_datasets.core.utils.read_config.ReadConfig\n",
            "    builtins.str(builtins.object)\n",
            "        tensorflow_datasets.core.splits.Split\n",
            "    enum.Enum(builtins.object)\n",
            "        tensorflow_datasets.core.download.util.GenerateMode\n",
            "    tensorflow_datasets.core.dataset_builder.DatasetBuilder(tensorflow_datasets.core.registered.RegisteredDataset)\n",
            "        tensorflow_datasets.core.folder_dataset.image_folder.ImageFolder\n",
            "        tensorflow_datasets.core.folder_dataset.translate_folder.TranslateFolder\n",
            "    \n",
            "    class GenerateMode(enum.Enum)\n",
            "     |  GenerateMode(value, names=None, *, module=None, qualname=None, type=None, start=1)\n",
            "     |  \n",
            "     |  `Enum` for how to treat pre-existing downloads and data.\n",
            "     |  \n",
            "     |  The default mode is `REUSE_DATASET_IF_EXISTS`, which will reuse both\n",
            "     |  raw downloads and the prepared dataset if they exist.\n",
            "     |  \n",
            "     |  The generations modes:\n",
            "     |  \n",
            "     |  |                                    | Downloads | Dataset | Metadata |\n",
            "     |  | -----------------------------------|-----------|---------|----------|\n",
            "     |  | `REUSE_DATASET_IF_EXISTS` (default)| Reuse     | Reuse   | Reuse    |\n",
            "     |  | `UPDATE_DATASET_INFO`              | Reuse     | Reuse   | Fresh    |\n",
            "     |  | `REUSE_CACHE_IF_EXISTS`            | Reuse     | Fresh   | Fresh    |\n",
            "     |  | `FORCE_REDOWNLOAD`                 | Fresh     | Fresh   | Fresh    |\n",
            "     |  \n",
            "     |  UPDATE_DATASET_INFO only regenerates DatasetInfo metadata which is directly\n",
            "     |  coming from the Builder metadata, and not directly used to prepare the data\n",
            "     |  or computed from the downloaded or prepared data.\n",
            "     |  This means that `description`, `config_tags`, etc. will be updated, but\n",
            "     |  `download_size`, `schema`, `splits`, `disable_shuffling`, `file_format` will\n",
            "     |  not be updated.\n",
            "     |  UPDATE_DATASET_INFO will fail if the data has never been prepared.\n",
            "     |  \n",
            "     |  Method resolution order:\n",
            "     |      GenerateMode\n",
            "     |      enum.Enum\n",
            "     |      builtins.object\n",
            "     |  \n",
            "     |  Data and other attributes defined here:\n",
            "     |  \n",
            "     |  FORCE_REDOWNLOAD = <GenerateMode.FORCE_REDOWNLOAD: 'force_redownload'>\n",
            "     |  \n",
            "     |  REUSE_CACHE_IF_EXISTS = <GenerateMode.REUSE_CACHE_IF_EXISTS: 'reuse_ca...\n",
            "     |  \n",
            "     |  REUSE_DATASET_IF_EXISTS = <GenerateMode.REUSE_DATASET_IF_EXISTS: 'reus...\n",
            "     |  \n",
            "     |  UPDATE_DATASET_INFO = <GenerateMode.UPDATE_DATASET_INFO: 'update_datas...\n",
            "     |  \n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Data descriptors inherited from enum.Enum:\n",
            "     |  \n",
            "     |  name\n",
            "     |      The name of the Enum member.\n",
            "     |  \n",
            "     |  value\n",
            "     |      The value of the Enum member.\n",
            "     |  \n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Readonly properties inherited from enum.EnumMeta:\n",
            "     |  \n",
            "     |  __members__\n",
            "     |      Returns a mapping of member name->value.\n",
            "     |      \n",
            "     |      This mapping lists all enum members, including aliases. Note that this\n",
            "     |      is a read-only view of the internal mapping.\n",
            "    \n",
            "    class ImageFolder(tensorflow_datasets.core.dataset_builder.DatasetBuilder)\n",
            "     |  ImageFolder(root_dir: 'str', *, shape: 'Optional[type_utils.Shape]' = None, dtype: 'Optional[tf.DType]' = None)\n",
            "     |  \n",
            "     |  Generic image classification dataset created from manual directory.\n",
            "     |  \n",
            "     |  `ImageFolder` creates a `tf.data.Dataset` reading the original image files.\n",
            "     |  \n",
            "     |  The data directory should have the following structure:\n",
            "     |  \n",
            "     |  ```\n",
            "     |  path/to/image_dir/\n",
            "     |    split_name/  # Ex: 'train'\n",
            "     |      label1/  # Ex: 'airplane' or '0015'\n",
            "     |        xxx.png\n",
            "     |        xxy.png\n",
            "     |        xxz.png\n",
            "     |      label2/\n",
            "     |        xxx.png\n",
            "     |        xxy.png\n",
            "     |        xxz.png\n",
            "     |    split_name/  # Ex: 'test'\n",
            "     |      ...\n",
            "     |  ```\n",
            "     |  \n",
            "     |  To use it:\n",
            "     |  \n",
            "     |  ```\n",
            "     |  builder = tfds.ImageFolder('path/to/image_dir/')\n",
            "     |  print(builder.info)  # num examples, labels... are automatically calculated\n",
            "     |  ds = builder.as_dataset(split='train', shuffle_files=True)\n",
            "     |  tfds.show_examples(ds, builder.info)\n",
            "     |  ```\n",
            "     |  \n",
            "     |  Method resolution order:\n",
            "     |      ImageFolder\n",
            "     |      tensorflow_datasets.core.dataset_builder.DatasetBuilder\n",
            "     |      tensorflow_datasets.core.registered.RegisteredDataset\n",
            "     |      abc.ABC\n",
            "     |      builtins.object\n",
            "     |  \n",
            "     |  Methods defined here:\n",
            "     |  \n",
            "     |  __init__(self, root_dir: 'str', *, shape: 'Optional[type_utils.Shape]' = None, dtype: 'Optional[tf.DType]' = None)\n",
            "     |      Construct the `DatasetBuilder`.\n",
            "     |      \n",
            "     |      Args:\n",
            "     |        root_dir: Path to the directory containing the images.\n",
            "     |        shape: Image shape forwarded to `tfds.features.Image`.\n",
            "     |        dtype: Image dtype forwarded to `tfds.features.Image`.\n",
            "     |  \n",
            "     |  download_and_prepare(self, **kwargs)\n",
            "     |      Downloads and prepares dataset for reading.\n",
            "     |      \n",
            "     |      Args:\n",
            "     |        download_dir: directory where downloaded files are stored. Defaults to\n",
            "     |          \"~/tensorflow-datasets/downloads\".\n",
            "     |        download_config: `tfds.download.DownloadConfig`, further configuration for\n",
            "     |          downloading and preparing dataset.\n",
            "     |        file_format: optional `str` or `file_adapters.FileFormat`, format of the\n",
            "     |          record files in which the dataset will be written.\n",
            "     |      \n",
            "     |      Raises:\n",
            "     |        IOError: if there is not enough disk space available.\n",
            "     |        RuntimeError: when the config cannot be found.\n",
            "     |  \n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Data and other attributes defined here:\n",
            "     |  \n",
            "     |  VERSION = Version('1.0.0')\n",
            "     |  \n",
            "     |  __abstractmethods__ = frozenset()\n",
            "     |  \n",
            "     |  __annotations__ = {}\n",
            "     |  \n",
            "     |  name = 'image_folder'\n",
            "     |  \n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Methods inherited from tensorflow_datasets.core.dataset_builder.DatasetBuilder:\n",
            "     |  \n",
            "     |  __getstate__(self)\n",
            "     |  \n",
            "     |  __setstate__(self, state)\n",
            "     |  \n",
            "     |  as_data_source(self, split: 'Optional[Tree[splits_lib.SplitArg]]' = None, *, decoders: 'Optional[TreeDict[decode.partial_decode.DecoderArg]]' = None) -> 'ListOrTreeOrElem[Sequence[Any]]'\n",
            "     |      Constructs an `ArrayRecordDataSource`.\n",
            "     |      \n",
            "     |      Args:\n",
            "     |        split: Which split of the data to load (e.g. `'train'`, `'test'`,\n",
            "     |          `['train', 'test']`, `'train[80%:]'`,...). See our [split API\n",
            "     |          guide](https://www.tensorflow.org/datasets/splits). If `None`, will\n",
            "     |          return all splits in a `Dict[Split, Sequence]`.\n",
            "     |        decoders: Nested dict of `Decoder` objects which allow to customize the\n",
            "     |          decoding. The structure should match the feature structure, but only\n",
            "     |          customized feature keys need to be present. See [the\n",
            "     |          guide](https://github.com/tensorflow/datasets/blob/master/docs/decode.md)\n",
            "     |          for more info.\n",
            "     |      \n",
            "     |      Returns:\n",
            "     |        `Sequence` if `split`,\n",
            "     |        `dict<key: tfds.Split, value: Sequence>` otherwise.\n",
            "     |      \n",
            "     |      Raises:\n",
            "     |        NotImplementedError if the data was not generated using ArrayRecords.\n",
            "     |  \n",
            "     |  as_dataset(self, split: 'Optional[Tree[splits_lib.SplitArg]]' = None, *, batch_size: 'Optional[int]' = None, shuffle_files: 'bool' = False, decoders: 'Optional[TreeDict[decode.partial_decode.DecoderArg]]' = None, read_config: 'Optional[read_config_lib.ReadConfig]' = None, as_supervised: 'bool' = False)\n",
            "     |      Constructs a `tf.data.Dataset`.\n",
            "     |      \n",
            "     |      Callers must pass arguments as keyword arguments.\n",
            "     |      \n",
            "     |      The output types vary depending on the parameters. Examples:\n",
            "     |      \n",
            "     |      ```python\n",
            "     |      builder = tfds.builder('imdb_reviews')\n",
            "     |      builder.download_and_prepare()\n",
            "     |      \n",
            "     |      # Default parameters: Returns the dict of tf.data.Dataset\n",
            "     |      ds_all_dict = builder.as_dataset()\n",
            "     |      assert isinstance(ds_all_dict, dict)\n",
            "     |      print(ds_all_dict.keys())  # ==> ['test', 'train', 'unsupervised']\n",
            "     |      \n",
            "     |      assert isinstance(ds_all_dict['test'], tf.data.Dataset)\n",
            "     |      # Each dataset (test, train, unsup.) consists of dictionaries\n",
            "     |      # {'label': <tf.Tensor: .. dtype=int64, numpy=1>,\n",
            "     |      #  'text': <tf.Tensor: .. dtype=string, numpy=b\"I've watched the movie ..\">}\n",
            "     |      # {'label': <tf.Tensor: .. dtype=int64, numpy=1>,\n",
            "     |      #  'text': <tf.Tensor: .. dtype=string, numpy=b'If you love Japanese ..'>}\n",
            "     |      \n",
            "     |      # With as_supervised: tf.data.Dataset only contains (feature, label) tuples\n",
            "     |      ds_all_supervised = builder.as_dataset(as_supervised=True)\n",
            "     |      assert isinstance(ds_all_supervised, dict)\n",
            "     |      print(ds_all_supervised.keys())  # ==> ['test', 'train', 'unsupervised']\n",
            "     |      \n",
            "     |      assert isinstance(ds_all_supervised['test'], tf.data.Dataset)\n",
            "     |      # Each dataset (test, train, unsup.) consists of tuples (text, label)\n",
            "     |      # (<tf.Tensor: ... dtype=string, numpy=b\"I've watched the movie ..\">,\n",
            "     |      #  <tf.Tensor: ... dtype=int64, numpy=1>)\n",
            "     |      # (<tf.Tensor: ... dtype=string, numpy=b\"If you love Japanese ..\">,\n",
            "     |      #  <tf.Tensor: ... dtype=int64, numpy=1>)\n",
            "     |      \n",
            "     |      # Same as above plus requesting a particular split\n",
            "     |      ds_test_supervised = builder.as_dataset(as_supervised=True, split='test')\n",
            "     |      assert isinstance(ds_test_supervised, tf.data.Dataset)\n",
            "     |      # The dataset consists of tuples (text, label)\n",
            "     |      # (<tf.Tensor: ... dtype=string, numpy=b\"I've watched the movie ..\">,\n",
            "     |      #  <tf.Tensor: ... dtype=int64, numpy=1>)\n",
            "     |      # (<tf.Tensor: ... dtype=string, numpy=b\"If you love Japanese ..\">,\n",
            "     |      #  <tf.Tensor: ... dtype=int64, numpy=1>)\n",
            "     |      ```\n",
            "     |      \n",
            "     |      Args:\n",
            "     |        split: Which split of the data to load (e.g. `'train'`, `'test'`,\n",
            "     |          `['train', 'test']`, `'train[80%:]'`,...). See our [split API\n",
            "     |          guide](https://www.tensorflow.org/datasets/splits). If `None`, will\n",
            "     |          return all splits in a `Dict[Split, tf.data.Dataset]`.\n",
            "     |        batch_size: `int`, batch size. Note that variable-length features will be\n",
            "     |          0-padded if `batch_size` is set. Users that want more custom behavior\n",
            "     |          should use `batch_size=None` and use the `tf.data` API to construct a\n",
            "     |          custom pipeline. If `batch_size == -1`, will return feature dictionaries\n",
            "     |          of the whole dataset with `tf.Tensor`s instead of a `tf.data.Dataset`.\n",
            "     |        shuffle_files: `bool`, whether to shuffle the input files. Defaults to\n",
            "     |          `False`.\n",
            "     |        decoders: Nested dict of `Decoder` objects which allow to customize the\n",
            "     |          decoding. The structure should match the feature structure, but only\n",
            "     |          customized feature keys need to be present. See [the\n",
            "     |          guide](https://github.com/tensorflow/datasets/blob/master/docs/decode.md)\n",
            "     |          for more info.\n",
            "     |        read_config: `tfds.ReadConfig`, Additional options to configure the input\n",
            "     |          pipeline (e.g. seed, num parallel reads,...).\n",
            "     |        as_supervised: `bool`, if `True`, the returned `tf.data.Dataset` will have\n",
            "     |          a 2-tuple structure `(input, label)` according to\n",
            "     |          `builder.info.supervised_keys`. If `False`, the default, the returned\n",
            "     |          `tf.data.Dataset` will have a dictionary with all the features.\n",
            "     |      \n",
            "     |      Returns:\n",
            "     |        `tf.data.Dataset`, or if `split=None`, `dict<key: tfds.Split, value:\n",
            "     |        tf.data.Dataset>`.\n",
            "     |      \n",
            "     |        If `batch_size` is -1, will return feature dictionaries containing\n",
            "     |        the entire dataset in `tf.Tensor`s instead of a `tf.data.Dataset`.\n",
            "     |  \n",
            "     |  canonical_version = <functools.cached_property object>\n",
            "     |  dataset_info_from_configs(self, **kwargs)\n",
            "     |      Returns the DatasetInfo using given kwargs and config files.\n",
            "     |      \n",
            "     |      Sub-class should call this and add information not present in config files\n",
            "     |      using kwargs directly passed to tfds.core.DatasetInfo object.\n",
            "     |      \n",
            "     |      If information is present both in passed arguments and config files, config\n",
            "     |      files will prevail.\n",
            "     |      \n",
            "     |      Args:\n",
            "     |        **kwargs: kw args to pass to DatasetInfo directly.\n",
            "     |  \n",
            "     |  get_default_builder_config(self) -> 'Optional[BuilderConfig]'\n",
            "     |      Returns the default builder config if there is one.\n",
            "     |      \n",
            "     |      Note that for dataset builders that cannot use the `cls.BUILDER_CONFIGS`, we\n",
            "     |      need a method that uses the instance to get `BUILDER_CONFIGS` and\n",
            "     |      `DEFAULT_BUILDER_CONFIG_NAME`.\n",
            "     |      \n",
            "     |      Returns:\n",
            "     |        the default builder config if there is one\n",
            "     |  \n",
            "     |  get_reference(self, namespace: 'Optional[str]' = None) -> 'naming.DatasetReference'\n",
            "     |      Returns a reference to the dataset produced by this dataset builder.\n",
            "     |      \n",
            "     |      Includes the config if specified, the version, and the data_dir that should\n",
            "     |      contain this dataset.\n",
            "     |      \n",
            "     |      Arguments:\n",
            "     |        namespace: if this dataset is a community dataset, and therefore has a\n",
            "     |          namespace, then the namespace must be provided such that it can be set\n",
            "     |          in the reference. Note that a dataset builder is not aware that it is\n",
            "     |          part of a namespace.\n",
            "     |      \n",
            "     |      Returns:\n",
            "     |        a reference to this instantiated builder.\n",
            "     |  \n",
            "     |  is_prepared(self) -> 'bool'\n",
            "     |      Returns whether this dataset is already downloaded and prepared.\n",
            "     |  \n",
            "     |  supported_versions = <functools.cached_property object>\n",
            "     |  versions = <functools.cached_property object>\n",
            "     |      Versions (canonical + availables), in preference order.\n",
            "     |  \n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Class methods inherited from tensorflow_datasets.core.dataset_builder.DatasetBuilder:\n",
            "     |  \n",
            "     |  get_metadata() -> 'dataset_metadata.DatasetMetadata' from abc.ABCMeta\n",
            "     |      Returns metadata (README, CITATIONS, ...) specified in config files.\n",
            "     |      \n",
            "     |      The config files are read from the same package where the DatasetBuilder has\n",
            "     |      been defined, so those metadata might be wrong for legacy builders.\n",
            "     |  \n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Readonly properties inherited from tensorflow_datasets.core.dataset_builder.DatasetBuilder:\n",
            "     |  \n",
            "     |  builder_config\n",
            "     |      `tfds.core.BuilderConfig` for this builder.\n",
            "     |  \n",
            "     |  builder_config_cls\n",
            "     |      Returns the builder config class.\n",
            "     |  \n",
            "     |  builder_configs\n",
            "     |      Pre-defined list of configurations for this builder class.\n",
            "     |  \n",
            "     |  code_path\n",
            "     |      Returns the path to the file where the Dataset class is located.\n",
            "     |      \n",
            "     |      Note: As the code can be run inside zip file. The returned value is\n",
            "     |      a `Path` by default. Use `tfds.core.utils.to_write_path()` to cast\n",
            "     |      the path into `Path`.\n",
            "     |      \n",
            "     |      Returns:\n",
            "     |        path: pathlib.Path like abstraction\n",
            "     |  \n",
            "     |  data_dir\n",
            "     |      Returns the directory where this version + config is stored.\n",
            "     |      \n",
            "     |      Note that this is different from `data_dir_root`. For example, if\n",
            "     |      `data_dir_root` is `/data/tfds`, then `data_dir` would be\n",
            "     |      `/data/tfds/my_dataset/my_config/1.2.3`.\n",
            "     |      \n",
            "     |      Returns:\n",
            "     |        the directory where this version + config is stored.\n",
            "     |  \n",
            "     |  data_dir_root\n",
            "     |      Returns the root directory where all TFDS datasets are stored.\n",
            "     |      \n",
            "     |      Note that this is different from `data_dir`, which includes the dataset\n",
            "     |      name, config, and version. For example, if `data_dir` is\n",
            "     |      `/data/tfds/my_dataset/my_config/1.2.3`, then `data_dir_root` is\n",
            "     |      `/data/tfds`.\n",
            "     |      \n",
            "     |      Returns:\n",
            "     |        the root directory where all TFDS datasets are stored.\n",
            "     |  \n",
            "     |  data_path\n",
            "     |      Returns the path where this version + config is stored.\n",
            "     |  \n",
            "     |  default_builder_config\n",
            "     |  \n",
            "     |  info\n",
            "     |      `tfds.core.DatasetInfo` for this builder.\n",
            "     |  \n",
            "     |  release_notes\n",
            "     |  \n",
            "     |  url_infos\n",
            "     |      Load `UrlInfo` from the given path.\n",
            "     |  \n",
            "     |  version\n",
            "     |  \n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Data and other attributes inherited from tensorflow_datasets.core.dataset_builder.DatasetBuilder:\n",
            "     |  \n",
            "     |  BUILDER_CONFIGS = []\n",
            "     |  \n",
            "     |  DEFAULT_BUILDER_CONFIG_NAME = None\n",
            "     |  \n",
            "     |  MANUAL_DOWNLOAD_INSTRUCTIONS = None\n",
            "     |  \n",
            "     |  MAX_SIMULTANEOUS_DOWNLOADS = None\n",
            "     |  \n",
            "     |  RELEASE_NOTES = {}\n",
            "     |  \n",
            "     |  SUPPORTED_VERSIONS = []\n",
            "     |  \n",
            "     |  pkg_dir_path = None\n",
            "     |  \n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Class methods inherited from tensorflow_datasets.core.registered.RegisteredDataset:\n",
            "     |  \n",
            "     |  __init_subclass__(skip_registration=False, **kwargs) from abc.ABCMeta\n",
            "     |      This method is called when a class is subclassed.\n",
            "     |      \n",
            "     |      The default implementation does nothing. It may be\n",
            "     |      overridden to extend subclasses.\n",
            "     |  \n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Data descriptors inherited from tensorflow_datasets.core.registered.RegisteredDataset:\n",
            "     |  \n",
            "     |  __dict__\n",
            "     |      dictionary for instance variables (if defined)\n",
            "     |  \n",
            "     |  __weakref__\n",
            "     |      list of weak references to the object (if defined)\n",
            "    \n",
            "    class ReadConfig(builtins.object)\n",
            "     |  ReadConfig(options: 'Optional[tf.data.Options]' = None, try_autocache: 'bool' = True, repeat_filenames: 'bool' = False, add_tfds_id: 'bool' = False, shuffle_seed: 'Optional[int]' = None, shuffle_reshuffle_each_iteration: 'Optional[bool]' = None, interleave_cycle_length: 'Union[Optional[int], _MISSING]' = 'missing', interleave_block_length: 'Optional[int]' = 16, input_context: 'Optional[tf.distribute.InputContext]' = None, experimental_interleave_sort_fn: 'Optional[InterleaveSortFn]' = None, skip_prefetch: 'bool' = False, num_parallel_calls_for_decode: 'Optional[int]' = None, num_parallel_calls_for_interleave_files: 'Optional[int]' = 'missing', enable_ordering_guard: 'bool' = True, assert_cardinality: 'bool' = True, override_buffer_size: 'Optional[int]' = None) -> None\n",
            "     |  \n",
            "     |  Configures input reading pipeline.\n",
            "     |  \n",
            "     |  Attributes:\n",
            "     |    options: `tf.data.Options()`, dataset options to use. Note that when\n",
            "     |      `shuffle_files` is True and no seed is defined, deterministic will be set\n",
            "     |      to False internally, unless it is defined here.\n",
            "     |    try_autocache: If True (default) and the dataset satisfy the right\n",
            "     |      conditions (dataset small enough, files not shuffled,...) the dataset will\n",
            "     |      be cached during the first iteration (through `ds = ds.cache()`).\n",
            "     |    repeat_filenames: If True, repeat the filenames iterator. This will result\n",
            "     |      in an infinite dataset. Repeat is called after the shuffle of the\n",
            "     |      filenames.\n",
            "     |    add_tfds_id: If True, examples `dict` in `tf.data.Dataset` will have an\n",
            "     |      additional key `'tfds_id': tf.Tensor(shape=(), dtype=tf.string)`\n",
            "     |      containing the example unique identifier (e.g.\n",
            "     |      'train.tfrecord-000045-of-001024__123').\n",
            "     |       Note: IDs might changes in future version of TFDS.\n",
            "     |    shuffle_seed: `tf.int64`, seed forwarded to `tf.data.Dataset.shuffle` during\n",
            "     |      file shuffling (which happens when `tfds.load(..., shuffle_files=True)`).\n",
            "     |    shuffle_reshuffle_each_iteration: `bool`, forwarded to\n",
            "     |      `tf.data.Dataset.shuffle` during file shuffling (which happens when\n",
            "     |      `tfds.load(..., shuffle_files=True)`).\n",
            "     |    interleave_cycle_length: `int`, forwarded to `tf.data.Dataset.interleave`.\n",
            "     |    interleave_block_length: `int`, forwarded to `tf.data.Dataset.interleave`.\n",
            "     |    input_context: `tf.distribute.InputContext`, if set, each worker will read a\n",
            "     |      different set of file. For more info, see the\n",
            "     |      [distribute_datasets_from_function\n",
            "     |      documentation](https://www.tensorflow.org/api_docs/python/tf/distribute/Strategy#experimental_distribute_datasets_from_function).\n",
            "     |      Note:  * Each workers will always read the same subset of files.\n",
            "     |        `shuffle_files` only shuffle files within each worker. * If\n",
            "     |        `info.splits[split].num_shards < input_context.num_input_pipelines`, an\n",
            "     |        error will be raised, as some workers would be empty.\n",
            "     |    experimental_interleave_sort_fn: Function with signature `List[FileDict] ->\n",
            "     |      List[FileDict]`, which takes the list of `dict(file: str, take: int, skip:\n",
            "     |      int)` and returns the modified version to read. This can be used to\n",
            "     |      sort/shuffle the shards to read in a custom order, instead of relying on\n",
            "     |      `shuffle_files=True`.\n",
            "     |    skip_prefetch: If False (default), add a `ds.prefetch()` op at the end.\n",
            "     |      Might be set for performance optimization in some cases (e.g. if you're\n",
            "     |      already calling `ds.prefetch()` at the end of your pipeline)\n",
            "     |    num_parallel_calls_for_decode: The number of parallel calls for decoding\n",
            "     |      record. By default using tf.data's AUTOTUNE.\n",
            "     |    num_parallel_calls_for_interleave_files: The number of parallel calls for\n",
            "     |      interleaving files. By default using tf.data's AUTOTUNE.\n",
            "     |    enable_ordering_guard: When True (default), an exception is raised if\n",
            "     |      shuffling or interleaving are used on an ordered dataset.\n",
            "     |    assert_cardinality: When True (default), an exception is raised if at the\n",
            "     |      end of an Epoch the number of read examples does not match the expected\n",
            "     |      number from dataset metadata. A power user would typically want to set\n",
            "     |      False if input files have been tempered with and they don't mind missing\n",
            "     |      records or have too many of them.\n",
            "     |    override_buffer_size: number of bytes to pass to file readers for buffering.\n",
            "     |  \n",
            "     |  Methods defined here:\n",
            "     |  \n",
            "     |  __init__(self, options: 'Optional[tf.data.Options]' = None, try_autocache: 'bool' = True, repeat_filenames: 'bool' = False, add_tfds_id: 'bool' = False, shuffle_seed: 'Optional[int]' = None, shuffle_reshuffle_each_iteration: 'Optional[bool]' = None, interleave_cycle_length: 'Union[Optional[int], _MISSING]' = 'missing', interleave_block_length: 'Optional[int]' = 16, input_context: 'Optional[tf.distribute.InputContext]' = None, experimental_interleave_sort_fn: 'Optional[InterleaveSortFn]' = None, skip_prefetch: 'bool' = False, num_parallel_calls_for_decode: 'Optional[int]' = None, num_parallel_calls_for_interleave_files: 'Optional[int]' = 'missing', enable_ordering_guard: 'bool' = True, assert_cardinality: 'bool' = True, override_buffer_size: 'Optional[int]' = None) -> None\n",
            "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
            "     |  \n",
            "     |  __post_init__(self)\n",
            "     |  \n",
            "     |  __repr__(self)\n",
            "     |      Return repr(self).\n",
            "     |  \n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Data descriptors defined here:\n",
            "     |  \n",
            "     |  __dict__\n",
            "     |      dictionary for instance variables (if defined)\n",
            "     |  \n",
            "     |  __weakref__\n",
            "     |      list of weak references to the object (if defined)\n",
            "     |  \n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Data and other attributes defined here:\n",
            "     |  \n",
            "     |  __annotations__ = {'add_tfds_id': 'bool', 'assert_cardinality': 'bool'...\n",
            "     |  \n",
            "     |  __dataclass_fields__ = {'add_tfds_id': Field(name='add_tfds_id',type='...\n",
            "     |  \n",
            "     |  __dataclass_params__ = _DataclassParams(init=True,repr=True,eq=False,o...\n",
            "     |  \n",
            "     |  __match_args__ = ('options', 'try_autocache', 'repeat_filenames', 'add...\n",
            "     |  \n",
            "     |  add_tfds_id = False\n",
            "     |  \n",
            "     |  assert_cardinality = True\n",
            "     |  \n",
            "     |  enable_ordering_guard = True\n",
            "     |  \n",
            "     |  experimental_interleave_sort_fn = None\n",
            "     |  \n",
            "     |  input_context = None\n",
            "     |  \n",
            "     |  interleave_block_length = 16\n",
            "     |  \n",
            "     |  interleave_cycle_length = 'missing'\n",
            "     |  \n",
            "     |  num_parallel_calls_for_decode = None\n",
            "     |  \n",
            "     |  num_parallel_calls_for_interleave_files = 'missing'\n",
            "     |  \n",
            "     |  options = None\n",
            "     |  \n",
            "     |  override_buffer_size = None\n",
            "     |  \n",
            "     |  repeat_filenames = False\n",
            "     |  \n",
            "     |  shuffle_reshuffle_each_iteration = None\n",
            "     |  \n",
            "     |  shuffle_seed = None\n",
            "     |  \n",
            "     |  skip_prefetch = False\n",
            "     |  \n",
            "     |  try_autocache = True\n",
            "    \n",
            "    class Split(builtins.str)\n",
            "     |  `Enum` for dataset splits.\n",
            "     |  \n",
            "     |  Datasets are typically split into different subsets to be used at various\n",
            "     |  stages of training and evaluation.\n",
            "     |  \n",
            "     |  * `TRAIN`: the training data.\n",
            "     |  * `VALIDATION`: the validation data. If present, this is typically used as\n",
            "     |    evaluation data while iterating on a model (e.g. changing hyperparameters,\n",
            "     |    model architecture, etc.).\n",
            "     |  * `TEST`: the testing data. This is the data to report metrics on. Typically\n",
            "     |    you do not want to use this during model iteration as you may overfit to it.\n",
            "     |  * `ALL`: All splits from the dataset merged together (`'train+test+...'`).\n",
            "     |  \n",
            "     |  See the\n",
            "     |  [guide on\n",
            "     |  splits](https://github.com/tensorflow/datasets/blob/master/docs/splits.md)\n",
            "     |  for more information.\n",
            "     |  \n",
            "     |  Method resolution order:\n",
            "     |      Split\n",
            "     |      builtins.str\n",
            "     |      builtins.object\n",
            "     |  \n",
            "     |  Methods defined here:\n",
            "     |  \n",
            "     |  __repr__(self) -> 'str'\n",
            "     |      Return repr(self).\n",
            "     |  \n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Data descriptors defined here:\n",
            "     |  \n",
            "     |  __dict__\n",
            "     |      dictionary for instance variables (if defined)\n",
            "     |  \n",
            "     |  __weakref__\n",
            "     |      list of weak references to the object (if defined)\n",
            "     |  \n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Data and other attributes defined here:\n",
            "     |  \n",
            "     |  ALL = Split('all')\n",
            "     |  \n",
            "     |  TEST = Split('test')\n",
            "     |  \n",
            "     |  TRAIN = Split('train')\n",
            "     |  \n",
            "     |  VALIDATION = Split('validation')\n",
            "     |  \n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Methods inherited from builtins.str:\n",
            "     |  \n",
            "     |  __add__(self, value, /)\n",
            "     |      Return self+value.\n",
            "     |  \n",
            "     |  __contains__(self, key, /)\n",
            "     |      Return key in self.\n",
            "     |  \n",
            "     |  __eq__(self, value, /)\n",
            "     |      Return self==value.\n",
            "     |  \n",
            "     |  __format__(self, format_spec, /)\n",
            "     |      Return a formatted version of the string as described by format_spec.\n",
            "     |  \n",
            "     |  __ge__(self, value, /)\n",
            "     |      Return self>=value.\n",
            "     |  \n",
            "     |  __getattribute__(self, name, /)\n",
            "     |      Return getattr(self, name).\n",
            "     |  \n",
            "     |  __getitem__(self, key, /)\n",
            "     |      Return self[key].\n",
            "     |  \n",
            "     |  __getnewargs__(...)\n",
            "     |  \n",
            "     |  __gt__(self, value, /)\n",
            "     |      Return self>value.\n",
            "     |  \n",
            "     |  __hash__(self, /)\n",
            "     |      Return hash(self).\n",
            "     |  \n",
            "     |  __iter__(self, /)\n",
            "     |      Implement iter(self).\n",
            "     |  \n",
            "     |  __le__(self, value, /)\n",
            "     |      Return self<=value.\n",
            "     |  \n",
            "     |  __len__(self, /)\n",
            "     |      Return len(self).\n",
            "     |  \n",
            "     |  __lt__(self, value, /)\n",
            "     |      Return self<value.\n",
            "     |  \n",
            "     |  __mod__(self, value, /)\n",
            "     |      Return self%value.\n",
            "     |  \n",
            "     |  __mul__(self, value, /)\n",
            "     |      Return self*value.\n",
            "     |  \n",
            "     |  __ne__(self, value, /)\n",
            "     |      Return self!=value.\n",
            "     |  \n",
            "     |  __rmod__(self, value, /)\n",
            "     |      Return value%self.\n",
            "     |  \n",
            "     |  __rmul__(self, value, /)\n",
            "     |      Return value*self.\n",
            "     |  \n",
            "     |  __sizeof__(self, /)\n",
            "     |      Return the size of the string in memory, in bytes.\n",
            "     |  \n",
            "     |  __str__(self, /)\n",
            "     |      Return str(self).\n",
            "     |  \n",
            "     |  capitalize(self, /)\n",
            "     |      Return a capitalized version of the string.\n",
            "     |      \n",
            "     |      More specifically, make the first character have upper case and the rest lower\n",
            "     |      case.\n",
            "     |  \n",
            "     |  casefold(self, /)\n",
            "     |      Return a version of the string suitable for caseless comparisons.\n",
            "     |  \n",
            "     |  center(self, width, fillchar=' ', /)\n",
            "     |      Return a centered string of length width.\n",
            "     |      \n",
            "     |      Padding is done using the specified fill character (default is a space).\n",
            "     |  \n",
            "     |  count(...)\n",
            "     |      S.count(sub[, start[, end]]) -> int\n",
            "     |      \n",
            "     |      Return the number of non-overlapping occurrences of substring sub in\n",
            "     |      string S[start:end].  Optional arguments start and end are\n",
            "     |      interpreted as in slice notation.\n",
            "     |  \n",
            "     |  encode(self, /, encoding='utf-8', errors='strict')\n",
            "     |      Encode the string using the codec registered for encoding.\n",
            "     |      \n",
            "     |      encoding\n",
            "     |        The encoding in which to encode the string.\n",
            "     |      errors\n",
            "     |        The error handling scheme to use for encoding errors.\n",
            "     |        The default is 'strict' meaning that encoding errors raise a\n",
            "     |        UnicodeEncodeError.  Other possible values are 'ignore', 'replace' and\n",
            "     |        'xmlcharrefreplace' as well as any other name registered with\n",
            "     |        codecs.register_error that can handle UnicodeEncodeErrors.\n",
            "     |  \n",
            "     |  endswith(...)\n",
            "     |      S.endswith(suffix[, start[, end]]) -> bool\n",
            "     |      \n",
            "     |      Return True if S ends with the specified suffix, False otherwise.\n",
            "     |      With optional start, test S beginning at that position.\n",
            "     |      With optional end, stop comparing S at that position.\n",
            "     |      suffix can also be a tuple of strings to try.\n",
            "     |  \n",
            "     |  expandtabs(self, /, tabsize=8)\n",
            "     |      Return a copy where all tab characters are expanded using spaces.\n",
            "     |      \n",
            "     |      If tabsize is not given, a tab size of 8 characters is assumed.\n",
            "     |  \n",
            "     |  find(...)\n",
            "     |      S.find(sub[, start[, end]]) -> int\n",
            "     |      \n",
            "     |      Return the lowest index in S where substring sub is found,\n",
            "     |      such that sub is contained within S[start:end].  Optional\n",
            "     |      arguments start and end are interpreted as in slice notation.\n",
            "     |      \n",
            "     |      Return -1 on failure.\n",
            "     |  \n",
            "     |  format(...)\n",
            "     |      S.format(*args, **kwargs) -> str\n",
            "     |      \n",
            "     |      Return a formatted version of S, using substitutions from args and kwargs.\n",
            "     |      The substitutions are identified by braces ('{' and '}').\n",
            "     |  \n",
            "     |  format_map(...)\n",
            "     |      S.format_map(mapping) -> str\n",
            "     |      \n",
            "     |      Return a formatted version of S, using substitutions from mapping.\n",
            "     |      The substitutions are identified by braces ('{' and '}').\n",
            "     |  \n",
            "     |  index(...)\n",
            "     |      S.index(sub[, start[, end]]) -> int\n",
            "     |      \n",
            "     |      Return the lowest index in S where substring sub is found,\n",
            "     |      such that sub is contained within S[start:end].  Optional\n",
            "     |      arguments start and end are interpreted as in slice notation.\n",
            "     |      \n",
            "     |      Raises ValueError when the substring is not found.\n",
            "     |  \n",
            "     |  isalnum(self, /)\n",
            "     |      Return True if the string is an alpha-numeric string, False otherwise.\n",
            "     |      \n",
            "     |      A string is alpha-numeric if all characters in the string are alpha-numeric and\n",
            "     |      there is at least one character in the string.\n",
            "     |  \n",
            "     |  isalpha(self, /)\n",
            "     |      Return True if the string is an alphabetic string, False otherwise.\n",
            "     |      \n",
            "     |      A string is alphabetic if all characters in the string are alphabetic and there\n",
            "     |      is at least one character in the string.\n",
            "     |  \n",
            "     |  isascii(self, /)\n",
            "     |      Return True if all characters in the string are ASCII, False otherwise.\n",
            "     |      \n",
            "     |      ASCII characters have code points in the range U+0000-U+007F.\n",
            "     |      Empty string is ASCII too.\n",
            "     |  \n",
            "     |  isdecimal(self, /)\n",
            "     |      Return True if the string is a decimal string, False otherwise.\n",
            "     |      \n",
            "     |      A string is a decimal string if all characters in the string are decimal and\n",
            "     |      there is at least one character in the string.\n",
            "     |  \n",
            "     |  isdigit(self, /)\n",
            "     |      Return True if the string is a digit string, False otherwise.\n",
            "     |      \n",
            "     |      A string is a digit string if all characters in the string are digits and there\n",
            "     |      is at least one character in the string.\n",
            "     |  \n",
            "     |  isidentifier(self, /)\n",
            "     |      Return True if the string is a valid Python identifier, False otherwise.\n",
            "     |      \n",
            "     |      Call keyword.iskeyword(s) to test whether string s is a reserved identifier,\n",
            "     |      such as \"def\" or \"class\".\n",
            "     |  \n",
            "     |  islower(self, /)\n",
            "     |      Return True if the string is a lowercase string, False otherwise.\n",
            "     |      \n",
            "     |      A string is lowercase if all cased characters in the string are lowercase and\n",
            "     |      there is at least one cased character in the string.\n",
            "     |  \n",
            "     |  isnumeric(self, /)\n",
            "     |      Return True if the string is a numeric string, False otherwise.\n",
            "     |      \n",
            "     |      A string is numeric if all characters in the string are numeric and there is at\n",
            "     |      least one character in the string.\n",
            "     |  \n",
            "     |  isprintable(self, /)\n",
            "     |      Return True if the string is printable, False otherwise.\n",
            "     |      \n",
            "     |      A string is printable if all of its characters are considered printable in\n",
            "     |      repr() or if it is empty.\n",
            "     |  \n",
            "     |  isspace(self, /)\n",
            "     |      Return True if the string is a whitespace string, False otherwise.\n",
            "     |      \n",
            "     |      A string is whitespace if all characters in the string are whitespace and there\n",
            "     |      is at least one character in the string.\n",
            "     |  \n",
            "     |  istitle(self, /)\n",
            "     |      Return True if the string is a title-cased string, False otherwise.\n",
            "     |      \n",
            "     |      In a title-cased string, upper- and title-case characters may only\n",
            "     |      follow uncased characters and lowercase characters only cased ones.\n",
            "     |  \n",
            "     |  isupper(self, /)\n",
            "     |      Return True if the string is an uppercase string, False otherwise.\n",
            "     |      \n",
            "     |      A string is uppercase if all cased characters in the string are uppercase and\n",
            "     |      there is at least one cased character in the string.\n",
            "     |  \n",
            "     |  join(self, iterable, /)\n",
            "     |      Concatenate any number of strings.\n",
            "     |      \n",
            "     |      The string whose method is called is inserted in between each given string.\n",
            "     |      The result is returned as a new string.\n",
            "     |      \n",
            "     |      Example: '.'.join(['ab', 'pq', 'rs']) -> 'ab.pq.rs'\n",
            "     |  \n",
            "     |  ljust(self, width, fillchar=' ', /)\n",
            "     |      Return a left-justified string of length width.\n",
            "     |      \n",
            "     |      Padding is done using the specified fill character (default is a space).\n",
            "     |  \n",
            "     |  lower(self, /)\n",
            "     |      Return a copy of the string converted to lowercase.\n",
            "     |  \n",
            "     |  lstrip(self, chars=None, /)\n",
            "     |      Return a copy of the string with leading whitespace removed.\n",
            "     |      \n",
            "     |      If chars is given and not None, remove characters in chars instead.\n",
            "     |  \n",
            "     |  partition(self, sep, /)\n",
            "     |      Partition the string into three parts using the given separator.\n",
            "     |      \n",
            "     |      This will search for the separator in the string.  If the separator is found,\n",
            "     |      returns a 3-tuple containing the part before the separator, the separator\n",
            "     |      itself, and the part after it.\n",
            "     |      \n",
            "     |      If the separator is not found, returns a 3-tuple containing the original string\n",
            "     |      and two empty strings.\n",
            "     |  \n",
            "     |  removeprefix(self, prefix, /)\n",
            "     |      Return a str with the given prefix string removed if present.\n",
            "     |      \n",
            "     |      If the string starts with the prefix string, return string[len(prefix):].\n",
            "     |      Otherwise, return a copy of the original string.\n",
            "     |  \n",
            "     |  removesuffix(self, suffix, /)\n",
            "     |      Return a str with the given suffix string removed if present.\n",
            "     |      \n",
            "     |      If the string ends with the suffix string and that suffix is not empty,\n",
            "     |      return string[:-len(suffix)]. Otherwise, return a copy of the original\n",
            "     |      string.\n",
            "     |  \n",
            "     |  replace(self, old, new, count=-1, /)\n",
            "     |      Return a copy with all occurrences of substring old replaced by new.\n",
            "     |      \n",
            "     |        count\n",
            "     |          Maximum number of occurrences to replace.\n",
            "     |          -1 (the default value) means replace all occurrences.\n",
            "     |      \n",
            "     |      If the optional argument count is given, only the first count occurrences are\n",
            "     |      replaced.\n",
            "     |  \n",
            "     |  rfind(...)\n",
            "     |      S.rfind(sub[, start[, end]]) -> int\n",
            "     |      \n",
            "     |      Return the highest index in S where substring sub is found,\n",
            "     |      such that sub is contained within S[start:end].  Optional\n",
            "     |      arguments start and end are interpreted as in slice notation.\n",
            "     |      \n",
            "     |      Return -1 on failure.\n",
            "     |  \n",
            "     |  rindex(...)\n",
            "     |      S.rindex(sub[, start[, end]]) -> int\n",
            "     |      \n",
            "     |      Return the highest index in S where substring sub is found,\n",
            "     |      such that sub is contained within S[start:end].  Optional\n",
            "     |      arguments start and end are interpreted as in slice notation.\n",
            "     |      \n",
            "     |      Raises ValueError when the substring is not found.\n",
            "     |  \n",
            "     |  rjust(self, width, fillchar=' ', /)\n",
            "     |      Return a right-justified string of length width.\n",
            "     |      \n",
            "     |      Padding is done using the specified fill character (default is a space).\n",
            "     |  \n",
            "     |  rpartition(self, sep, /)\n",
            "     |      Partition the string into three parts using the given separator.\n",
            "     |      \n",
            "     |      This will search for the separator in the string, starting at the end. If\n",
            "     |      the separator is found, returns a 3-tuple containing the part before the\n",
            "     |      separator, the separator itself, and the part after it.\n",
            "     |      \n",
            "     |      If the separator is not found, returns a 3-tuple containing two empty strings\n",
            "     |      and the original string.\n",
            "     |  \n",
            "     |  rsplit(self, /, sep=None, maxsplit=-1)\n",
            "     |      Return a list of the substrings in the string, using sep as the separator string.\n",
            "     |      \n",
            "     |        sep\n",
            "     |          The separator used to split the string.\n",
            "     |      \n",
            "     |          When set to None (the default value), will split on any whitespace\n",
            "     |          character (including \\\\n \\\\r \\\\t \\\\f and spaces) and will discard\n",
            "     |          empty strings from the result.\n",
            "     |        maxsplit\n",
            "     |          Maximum number of splits (starting from the left).\n",
            "     |          -1 (the default value) means no limit.\n",
            "     |      \n",
            "     |      Splitting starts at the end of the string and works to the front.\n",
            "     |  \n",
            "     |  rstrip(self, chars=None, /)\n",
            "     |      Return a copy of the string with trailing whitespace removed.\n",
            "     |      \n",
            "     |      If chars is given and not None, remove characters in chars instead.\n",
            "     |  \n",
            "     |  split(self, /, sep=None, maxsplit=-1)\n",
            "     |      Return a list of the substrings in the string, using sep as the separator string.\n",
            "     |      \n",
            "     |        sep\n",
            "     |          The separator used to split the string.\n",
            "     |      \n",
            "     |          When set to None (the default value), will split on any whitespace\n",
            "     |          character (including \\\\n \\\\r \\\\t \\\\f and spaces) and will discard\n",
            "     |          empty strings from the result.\n",
            "     |        maxsplit\n",
            "     |          Maximum number of splits (starting from the left).\n",
            "     |          -1 (the default value) means no limit.\n",
            "     |      \n",
            "     |      Note, str.split() is mainly useful for data that has been intentionally\n",
            "     |      delimited.  With natural text that includes punctuation, consider using\n",
            "     |      the regular expression module.\n",
            "     |  \n",
            "     |  splitlines(self, /, keepends=False)\n",
            "     |      Return a list of the lines in the string, breaking at line boundaries.\n",
            "     |      \n",
            "     |      Line breaks are not included in the resulting list unless keepends is given and\n",
            "     |      true.\n",
            "     |  \n",
            "     |  startswith(...)\n",
            "     |      S.startswith(prefix[, start[, end]]) -> bool\n",
            "     |      \n",
            "     |      Return True if S starts with the specified prefix, False otherwise.\n",
            "     |      With optional start, test S beginning at that position.\n",
            "     |      With optional end, stop comparing S at that position.\n",
            "     |      prefix can also be a tuple of strings to try.\n",
            "     |  \n",
            "     |  strip(self, chars=None, /)\n",
            "     |      Return a copy of the string with leading and trailing whitespace removed.\n",
            "     |      \n",
            "     |      If chars is given and not None, remove characters in chars instead.\n",
            "     |  \n",
            "     |  swapcase(self, /)\n",
            "     |      Convert uppercase characters to lowercase and lowercase characters to uppercase.\n",
            "     |  \n",
            "     |  title(self, /)\n",
            "     |      Return a version of the string where each word is titlecased.\n",
            "     |      \n",
            "     |      More specifically, words start with uppercased characters and all remaining\n",
            "     |      cased characters have lower case.\n",
            "     |  \n",
            "     |  translate(self, table, /)\n",
            "     |      Replace each character in the string using the given translation table.\n",
            "     |      \n",
            "     |        table\n",
            "     |          Translation table, which must be a mapping of Unicode ordinals to\n",
            "     |          Unicode ordinals, strings, or None.\n",
            "     |      \n",
            "     |      The table must implement lookup/indexing via __getitem__, for instance a\n",
            "     |      dictionary or list.  If this operation raises LookupError, the character is\n",
            "     |      left untouched.  Characters mapped to None are deleted.\n",
            "     |  \n",
            "     |  upper(self, /)\n",
            "     |      Return a copy of the string converted to uppercase.\n",
            "     |  \n",
            "     |  zfill(self, width, /)\n",
            "     |      Pad a numeric string with zeros on the left, to fill a field of the given width.\n",
            "     |      \n",
            "     |      The string is never truncated.\n",
            "     |  \n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Static methods inherited from builtins.str:\n",
            "     |  \n",
            "     |  __new__(*args, **kwargs) from builtins.type\n",
            "     |      Create and return a new object.  See help(type) for accurate signature.\n",
            "     |  \n",
            "     |  maketrans(...)\n",
            "     |      Return a translation table usable for str.translate().\n",
            "     |      \n",
            "     |      If there is only one argument, it must be a dictionary mapping Unicode\n",
            "     |      ordinals (integers) or characters to Unicode ordinals, strings or None.\n",
            "     |      Character keys will be then converted to ordinals.\n",
            "     |      If there are two arguments, they must be strings of equal length, and\n",
            "     |      in the resulting dictionary, each character in x will be mapped to the\n",
            "     |      character at the same position in y. If there is a third argument, it\n",
            "     |      must be a string, whose characters will be mapped to None in the result.\n",
            "    \n",
            "    class TranslateFolder(tensorflow_datasets.core.dataset_builder.DatasetBuilder)\n",
            "     |  TranslateFolder(root_dir: 'str')\n",
            "     |  \n",
            "     |  Generic text translation dataset created from manual directory.\n",
            "     |  \n",
            "     |  The directory content should be as followed:\n",
            "     |  \n",
            "     |  ```\n",
            "     |  path/to/my_data/\n",
            "     |    lang1.train.txt\n",
            "     |    lang2.train.txt\n",
            "     |    lang1.test.txt\n",
            "     |    lang2.test.txt\n",
            "     |    ...\n",
            "     |  ```\n",
            "     |  \n",
            "     |  Each files should have one example per line. Line order should match between\n",
            "     |  files.\n",
            "     |  \n",
            "     |  To use it:\n",
            "     |  \n",
            "     |  ```\n",
            "     |  builder = tfds.TranslateFolder(root_dir='path/to/my_data/')\n",
            "     |  print(builder.info)  # Splits, num examples,... are automatically calculated\n",
            "     |  ds = builder.as_dataset(split='train', shuffle_files=True)\n",
            "     |  ```\n",
            "     |  \n",
            "     |  Note: All examples from all splits are loaded in memory in `__init__`.\n",
            "     |  \n",
            "     |  Method resolution order:\n",
            "     |      TranslateFolder\n",
            "     |      tensorflow_datasets.core.dataset_builder.DatasetBuilder\n",
            "     |      tensorflow_datasets.core.registered.RegisteredDataset\n",
            "     |      abc.ABC\n",
            "     |      builtins.object\n",
            "     |  \n",
            "     |  Methods defined here:\n",
            "     |  \n",
            "     |  __init__(self, root_dir: 'str')\n",
            "     |      Constructs a DatasetBuilder.\n",
            "     |      \n",
            "     |      Callers must pass arguments as keyword arguments.\n",
            "     |      \n",
            "     |      Args:\n",
            "     |        data_dir: directory to read/write data. Defaults to the value of the\n",
            "     |          environment variable TFDS_DATA_DIR, if set, otherwise falls back to\n",
            "     |          datasets are stored.\n",
            "     |        config: `tfds.core.BuilderConfig` or `str` name, optional configuration\n",
            "     |          for the dataset that affects the data generated on disk. Different\n",
            "     |          `builder_config`s will have their own subdirectories and versions.\n",
            "     |        version: Optional version at which to load the dataset. An error is raised\n",
            "     |          if specified version cannot be satisfied. Eg: '1.2.3', '1.2.*'. The\n",
            "     |          special value \"experimental_latest\" will use the highest version, even\n",
            "     |          if not default. This is not recommended unless you know what you are\n",
            "     |          doing, as the version could be broken.\n",
            "     |  \n",
            "     |  download_and_prepare(self, **kwargs)\n",
            "     |      Downloads and prepares dataset for reading.\n",
            "     |      \n",
            "     |      Args:\n",
            "     |        download_dir: directory where downloaded files are stored. Defaults to\n",
            "     |          \"~/tensorflow-datasets/downloads\".\n",
            "     |        download_config: `tfds.download.DownloadConfig`, further configuration for\n",
            "     |          downloading and preparing dataset.\n",
            "     |        file_format: optional `str` or `file_adapters.FileFormat`, format of the\n",
            "     |          record files in which the dataset will be written.\n",
            "     |      \n",
            "     |      Raises:\n",
            "     |        IOError: if there is not enough disk space available.\n",
            "     |        RuntimeError: when the config cannot be found.\n",
            "     |  \n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Data and other attributes defined here:\n",
            "     |  \n",
            "     |  VERSION = Version('1.0.0')\n",
            "     |  \n",
            "     |  __abstractmethods__ = frozenset()\n",
            "     |  \n",
            "     |  __annotations__ = {}\n",
            "     |  \n",
            "     |  name = 'translate_folder'\n",
            "     |  \n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Methods inherited from tensorflow_datasets.core.dataset_builder.DatasetBuilder:\n",
            "     |  \n",
            "     |  __getstate__(self)\n",
            "     |  \n",
            "     |  __setstate__(self, state)\n",
            "     |  \n",
            "     |  as_data_source(self, split: 'Optional[Tree[splits_lib.SplitArg]]' = None, *, decoders: 'Optional[TreeDict[decode.partial_decode.DecoderArg]]' = None) -> 'ListOrTreeOrElem[Sequence[Any]]'\n",
            "     |      Constructs an `ArrayRecordDataSource`.\n",
            "     |      \n",
            "     |      Args:\n",
            "     |        split: Which split of the data to load (e.g. `'train'`, `'test'`,\n",
            "     |          `['train', 'test']`, `'train[80%:]'`,...). See our [split API\n",
            "     |          guide](https://www.tensorflow.org/datasets/splits). If `None`, will\n",
            "     |          return all splits in a `Dict[Split, Sequence]`.\n",
            "     |        decoders: Nested dict of `Decoder` objects which allow to customize the\n",
            "     |          decoding. The structure should match the feature structure, but only\n",
            "     |          customized feature keys need to be present. See [the\n",
            "     |          guide](https://github.com/tensorflow/datasets/blob/master/docs/decode.md)\n",
            "     |          for more info.\n",
            "     |      \n",
            "     |      Returns:\n",
            "     |        `Sequence` if `split`,\n",
            "     |        `dict<key: tfds.Split, value: Sequence>` otherwise.\n",
            "     |      \n",
            "     |      Raises:\n",
            "     |        NotImplementedError if the data was not generated using ArrayRecords.\n",
            "     |  \n",
            "     |  as_dataset(self, split: 'Optional[Tree[splits_lib.SplitArg]]' = None, *, batch_size: 'Optional[int]' = None, shuffle_files: 'bool' = False, decoders: 'Optional[TreeDict[decode.partial_decode.DecoderArg]]' = None, read_config: 'Optional[read_config_lib.ReadConfig]' = None, as_supervised: 'bool' = False)\n",
            "     |      Constructs a `tf.data.Dataset`.\n",
            "     |      \n",
            "     |      Callers must pass arguments as keyword arguments.\n",
            "     |      \n",
            "     |      The output types vary depending on the parameters. Examples:\n",
            "     |      \n",
            "     |      ```python\n",
            "     |      builder = tfds.builder('imdb_reviews')\n",
            "     |      builder.download_and_prepare()\n",
            "     |      \n",
            "     |      # Default parameters: Returns the dict of tf.data.Dataset\n",
            "     |      ds_all_dict = builder.as_dataset()\n",
            "     |      assert isinstance(ds_all_dict, dict)\n",
            "     |      print(ds_all_dict.keys())  # ==> ['test', 'train', 'unsupervised']\n",
            "     |      \n",
            "     |      assert isinstance(ds_all_dict['test'], tf.data.Dataset)\n",
            "     |      # Each dataset (test, train, unsup.) consists of dictionaries\n",
            "     |      # {'label': <tf.Tensor: .. dtype=int64, numpy=1>,\n",
            "     |      #  'text': <tf.Tensor: .. dtype=string, numpy=b\"I've watched the movie ..\">}\n",
            "     |      # {'label': <tf.Tensor: .. dtype=int64, numpy=1>,\n",
            "     |      #  'text': <tf.Tensor: .. dtype=string, numpy=b'If you love Japanese ..'>}\n",
            "     |      \n",
            "     |      # With as_supervised: tf.data.Dataset only contains (feature, label) tuples\n",
            "     |      ds_all_supervised = builder.as_dataset(as_supervised=True)\n",
            "     |      assert isinstance(ds_all_supervised, dict)\n",
            "     |      print(ds_all_supervised.keys())  # ==> ['test', 'train', 'unsupervised']\n",
            "     |      \n",
            "     |      assert isinstance(ds_all_supervised['test'], tf.data.Dataset)\n",
            "     |      # Each dataset (test, train, unsup.) consists of tuples (text, label)\n",
            "     |      # (<tf.Tensor: ... dtype=string, numpy=b\"I've watched the movie ..\">,\n",
            "     |      #  <tf.Tensor: ... dtype=int64, numpy=1>)\n",
            "     |      # (<tf.Tensor: ... dtype=string, numpy=b\"If you love Japanese ..\">,\n",
            "     |      #  <tf.Tensor: ... dtype=int64, numpy=1>)\n",
            "     |      \n",
            "     |      # Same as above plus requesting a particular split\n",
            "     |      ds_test_supervised = builder.as_dataset(as_supervised=True, split='test')\n",
            "     |      assert isinstance(ds_test_supervised, tf.data.Dataset)\n",
            "     |      # The dataset consists of tuples (text, label)\n",
            "     |      # (<tf.Tensor: ... dtype=string, numpy=b\"I've watched the movie ..\">,\n",
            "     |      #  <tf.Tensor: ... dtype=int64, numpy=1>)\n",
            "     |      # (<tf.Tensor: ... dtype=string, numpy=b\"If you love Japanese ..\">,\n",
            "     |      #  <tf.Tensor: ... dtype=int64, numpy=1>)\n",
            "     |      ```\n",
            "     |      \n",
            "     |      Args:\n",
            "     |        split: Which split of the data to load (e.g. `'train'`, `'test'`,\n",
            "     |          `['train', 'test']`, `'train[80%:]'`,...). See our [split API\n",
            "     |          guide](https://www.tensorflow.org/datasets/splits). If `None`, will\n",
            "     |          return all splits in a `Dict[Split, tf.data.Dataset]`.\n",
            "     |        batch_size: `int`, batch size. Note that variable-length features will be\n",
            "     |          0-padded if `batch_size` is set. Users that want more custom behavior\n",
            "     |          should use `batch_size=None` and use the `tf.data` API to construct a\n",
            "     |          custom pipeline. If `batch_size == -1`, will return feature dictionaries\n",
            "     |          of the whole dataset with `tf.Tensor`s instead of a `tf.data.Dataset`.\n",
            "     |        shuffle_files: `bool`, whether to shuffle the input files. Defaults to\n",
            "     |          `False`.\n",
            "     |        decoders: Nested dict of `Decoder` objects which allow to customize the\n",
            "     |          decoding. The structure should match the feature structure, but only\n",
            "     |          customized feature keys need to be present. See [the\n",
            "     |          guide](https://github.com/tensorflow/datasets/blob/master/docs/decode.md)\n",
            "     |          for more info.\n",
            "     |        read_config: `tfds.ReadConfig`, Additional options to configure the input\n",
            "     |          pipeline (e.g. seed, num parallel reads,...).\n",
            "     |        as_supervised: `bool`, if `True`, the returned `tf.data.Dataset` will have\n",
            "     |          a 2-tuple structure `(input, label)` according to\n",
            "     |          `builder.info.supervised_keys`. If `False`, the default, the returned\n",
            "     |          `tf.data.Dataset` will have a dictionary with all the features.\n",
            "     |      \n",
            "     |      Returns:\n",
            "     |        `tf.data.Dataset`, or if `split=None`, `dict<key: tfds.Split, value:\n",
            "     |        tf.data.Dataset>`.\n",
            "     |      \n",
            "     |        If `batch_size` is -1, will return feature dictionaries containing\n",
            "     |        the entire dataset in `tf.Tensor`s instead of a `tf.data.Dataset`.\n",
            "     |  \n",
            "     |  canonical_version = <functools.cached_property object>\n",
            "     |  dataset_info_from_configs(self, **kwargs)\n",
            "     |      Returns the DatasetInfo using given kwargs and config files.\n",
            "     |      \n",
            "     |      Sub-class should call this and add information not present in config files\n",
            "     |      using kwargs directly passed to tfds.core.DatasetInfo object.\n",
            "     |      \n",
            "     |      If information is present both in passed arguments and config files, config\n",
            "     |      files will prevail.\n",
            "     |      \n",
            "     |      Args:\n",
            "     |        **kwargs: kw args to pass to DatasetInfo directly.\n",
            "     |  \n",
            "     |  get_default_builder_config(self) -> 'Optional[BuilderConfig]'\n",
            "     |      Returns the default builder config if there is one.\n",
            "     |      \n",
            "     |      Note that for dataset builders that cannot use the `cls.BUILDER_CONFIGS`, we\n",
            "     |      need a method that uses the instance to get `BUILDER_CONFIGS` and\n",
            "     |      `DEFAULT_BUILDER_CONFIG_NAME`.\n",
            "     |      \n",
            "     |      Returns:\n",
            "     |        the default builder config if there is one\n",
            "     |  \n",
            "     |  get_reference(self, namespace: 'Optional[str]' = None) -> 'naming.DatasetReference'\n",
            "     |      Returns a reference to the dataset produced by this dataset builder.\n",
            "     |      \n",
            "     |      Includes the config if specified, the version, and the data_dir that should\n",
            "     |      contain this dataset.\n",
            "     |      \n",
            "     |      Arguments:\n",
            "     |        namespace: if this dataset is a community dataset, and therefore has a\n",
            "     |          namespace, then the namespace must be provided such that it can be set\n",
            "     |          in the reference. Note that a dataset builder is not aware that it is\n",
            "     |          part of a namespace.\n",
            "     |      \n",
            "     |      Returns:\n",
            "     |        a reference to this instantiated builder.\n",
            "     |  \n",
            "     |  is_prepared(self) -> 'bool'\n",
            "     |      Returns whether this dataset is already downloaded and prepared.\n",
            "     |  \n",
            "     |  supported_versions = <functools.cached_property object>\n",
            "     |  versions = <functools.cached_property object>\n",
            "     |      Versions (canonical + availables), in preference order.\n",
            "     |  \n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Class methods inherited from tensorflow_datasets.core.dataset_builder.DatasetBuilder:\n",
            "     |  \n",
            "     |  get_metadata() -> 'dataset_metadata.DatasetMetadata' from abc.ABCMeta\n",
            "     |      Returns metadata (README, CITATIONS, ...) specified in config files.\n",
            "     |      \n",
            "     |      The config files are read from the same package where the DatasetBuilder has\n",
            "     |      been defined, so those metadata might be wrong for legacy builders.\n",
            "     |  \n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Readonly properties inherited from tensorflow_datasets.core.dataset_builder.DatasetBuilder:\n",
            "     |  \n",
            "     |  builder_config\n",
            "     |      `tfds.core.BuilderConfig` for this builder.\n",
            "     |  \n",
            "     |  builder_config_cls\n",
            "     |      Returns the builder config class.\n",
            "     |  \n",
            "     |  builder_configs\n",
            "     |      Pre-defined list of configurations for this builder class.\n",
            "     |  \n",
            "     |  code_path\n",
            "     |      Returns the path to the file where the Dataset class is located.\n",
            "     |      \n",
            "     |      Note: As the code can be run inside zip file. The returned value is\n",
            "     |      a `Path` by default. Use `tfds.core.utils.to_write_path()` to cast\n",
            "     |      the path into `Path`.\n",
            "     |      \n",
            "     |      Returns:\n",
            "     |        path: pathlib.Path like abstraction\n",
            "     |  \n",
            "     |  data_dir\n",
            "     |      Returns the directory where this version + config is stored.\n",
            "     |      \n",
            "     |      Note that this is different from `data_dir_root`. For example, if\n",
            "     |      `data_dir_root` is `/data/tfds`, then `data_dir` would be\n",
            "     |      `/data/tfds/my_dataset/my_config/1.2.3`.\n",
            "     |      \n",
            "     |      Returns:\n",
            "     |        the directory where this version + config is stored.\n",
            "     |  \n",
            "     |  data_dir_root\n",
            "     |      Returns the root directory where all TFDS datasets are stored.\n",
            "     |      \n",
            "     |      Note that this is different from `data_dir`, which includes the dataset\n",
            "     |      name, config, and version. For example, if `data_dir` is\n",
            "     |      `/data/tfds/my_dataset/my_config/1.2.3`, then `data_dir_root` is\n",
            "     |      `/data/tfds`.\n",
            "     |      \n",
            "     |      Returns:\n",
            "     |        the root directory where all TFDS datasets are stored.\n",
            "     |  \n",
            "     |  data_path\n",
            "     |      Returns the path where this version + config is stored.\n",
            "     |  \n",
            "     |  default_builder_config\n",
            "     |  \n",
            "     |  info\n",
            "     |      `tfds.core.DatasetInfo` for this builder.\n",
            "     |  \n",
            "     |  release_notes\n",
            "     |  \n",
            "     |  url_infos\n",
            "     |      Load `UrlInfo` from the given path.\n",
            "     |  \n",
            "     |  version\n",
            "     |  \n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Data and other attributes inherited from tensorflow_datasets.core.dataset_builder.DatasetBuilder:\n",
            "     |  \n",
            "     |  BUILDER_CONFIGS = []\n",
            "     |  \n",
            "     |  DEFAULT_BUILDER_CONFIG_NAME = None\n",
            "     |  \n",
            "     |  MANUAL_DOWNLOAD_INSTRUCTIONS = None\n",
            "     |  \n",
            "     |  MAX_SIMULTANEOUS_DOWNLOADS = None\n",
            "     |  \n",
            "     |  RELEASE_NOTES = {}\n",
            "     |  \n",
            "     |  SUPPORTED_VERSIONS = []\n",
            "     |  \n",
            "     |  pkg_dir_path = None\n",
            "     |  \n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Class methods inherited from tensorflow_datasets.core.registered.RegisteredDataset:\n",
            "     |  \n",
            "     |  __init_subclass__(skip_registration=False, **kwargs) from abc.ABCMeta\n",
            "     |      This method is called when a class is subclassed.\n",
            "     |      \n",
            "     |      The default implementation does nothing. It may be\n",
            "     |      overridden to extend subclasses.\n",
            "     |  \n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Data descriptors inherited from tensorflow_datasets.core.registered.RegisteredDataset:\n",
            "     |  \n",
            "     |  __dict__\n",
            "     |      dictionary for instance variables (if defined)\n",
            "     |  \n",
            "     |  __weakref__\n",
            "     |      list of weak references to the object (if defined)\n",
            "\n",
            "FUNCTIONS\n",
            "    as_dataframe(ds: 'tf.data.Dataset', ds_info: 'Optional[dataset_info.DatasetInfo]' = None) -> 'pd.DataFrame'\n",
            "        Convert the dataset into a pandas dataframe.\n",
            "        \n",
            "        Warning: The dataframe will be loaded entirely in memory, you may\n",
            "        want to call `tfds.as_dataframe` on a subset of the data instead:\n",
            "        \n",
            "        ```\n",
            "        df = tfds.as_dataframe(ds.take(10), ds_info)\n",
            "        ```\n",
            "        \n",
            "        Args:\n",
            "          ds: `tf.data.Dataset`. The tf.data.Dataset object to convert to panda\n",
            "            dataframe. Examples should not be batched. The full dataset will be\n",
            "            loaded.\n",
            "          ds_info: Dataset info object. If given, helps improving the formatting.\n",
            "            Available either through `tfds.load('mnist', with_info=True)` or\n",
            "            `tfds.builder('mnist').info`\n",
            "        \n",
            "        Returns:\n",
            "          dataframe: The `pandas.DataFrame` object\n",
            "    \n",
            "    as_numpy(dataset: 'Tree[TensorflowElem]') -> 'Tree[NumpyElem]'\n",
            "        Converts a `tf.data.Dataset` to an iterable of NumPy arrays.\n",
            "        \n",
            "        `as_numpy` converts a possibly nested structure of `tf.data.Dataset`s\n",
            "        and `tf.Tensor`s to iterables of NumPy arrays and NumPy arrays, respectively.\n",
            "        \n",
            "        Note that because TensorFlow has support for ragged tensors and NumPy has\n",
            "        no equivalent representation,\n",
            "        [`tf.RaggedTensor`s](https://www.tensorflow.org/api_docs/python/tf/RaggedTensor)\n",
            "        are left as-is for the user to deal with them (e.g. using `to_list()`).\n",
            "        In TF 1 (i.e. graph mode), `tf.RaggedTensor`s are returned as\n",
            "        `tf.ragged.RaggedTensorValue`s.\n",
            "        \n",
            "        Example:\n",
            "        \n",
            "        ```\n",
            "        ds = tfds.load(name=\"mnist\", split=\"train\")\n",
            "        ds_numpy = tfds.as_numpy(ds)  # Convert `tf.data.Dataset` to Python generator\n",
            "        for ex in ds_numpy:\n",
            "          # `{'image': np.array(shape=(28, 28, 1)), 'labels': np.array(shape=())}`\n",
            "          print(ex)\n",
            "        ```\n",
            "        \n",
            "        Args:\n",
            "          dataset: a possibly nested structure of `tf.data.Dataset`s and/or\n",
            "            `tf.Tensor`s.\n",
            "        \n",
            "        Returns:\n",
            "          A structure matching `dataset` where `tf.data.Dataset`s are converted to\n",
            "          generators of NumPy arrays and `tf.Tensor`s are converted to NumPy arrays.\n",
            "    \n",
            "    benchmark(ds: 'Iterable[Any]', *, num_iter: 'Optional[int]' = None, batch_size: 'int' = 1) -> 'BenchmarkResult'\n",
            "        Benchmarks any iterable (e.g `tf.data.Dataset`).\n",
            "        \n",
            "        Usage:\n",
            "        \n",
            "        ```py\n",
            "        ds = tfds.load('mnist', split='train')\n",
            "        ds = ds.batch(32).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
            "        tfds.benchmark(ds, batch_size=32)\n",
            "        ```\n",
            "        \n",
            "        Reports:\n",
            "        \n",
            "        - Total execution time\n",
            "        - Setup time (first warmup batch)\n",
            "        - Number of examples/sec\n",
            "        \n",
            "        Args:\n",
            "          ds: Dataset to benchmark. Can be any iterable. Note: The iterable will be\n",
            "            fully consumed.\n",
            "          num_iter: Number of iteration to perform (iteration might be batched)\n",
            "          batch_size: Batch size of the dataset, used to normalize iterations\n",
            "        \n",
            "        Returns:\n",
            "          statistics: The recorded statistics, for eventual post-processing\n",
            "    \n",
            "    builder(name: 'str', *, try_gcs: 'bool' = False, **builder_kwargs: 'Any') -> 'dataset_builder.DatasetBuilder'\n",
            "        Fetches a `tfds.core.DatasetBuilder` by string name.\n",
            "        \n",
            "        Args:\n",
            "          name: `str`, the registered name of the `DatasetBuilder` (the class name as\n",
            "            camel or snake case: `MyDataset` or `my_dataset`). This can be either\n",
            "            `'dataset_name'` or `'dataset_name/config_name'` for datasets with\n",
            "            `BuilderConfig`s. As a convenience, this string may contain\n",
            "            comma-separated keyword arguments for the builder. For example\n",
            "            `'foo_bar/a=True,b=3'` would use the `FooBar` dataset passing the keyword\n",
            "            arguments `a=True` and `b=3` (for builders with configs, it would be\n",
            "            `'foo_bar/zoo/a=True,b=3'` to use the `'zoo'` config and pass to the\n",
            "            builder keyword arguments `a=True` and `b=3`).\n",
            "          try_gcs: `bool`, if True, `tfds.load` will see if the dataset exists on the\n",
            "            public GCS bucket before building it locally. This is equivalent to\n",
            "            passing `data_dir='gs://tfds-data/datasets'`. Warning: `try_gcs` is\n",
            "            different than `builder_kwargs.download_config.try_download_gcs`.\n",
            "            `try_gcs` (default: False) overrides `data_dir` to be the public GCS\n",
            "            bucket. `try_download_gcs` (default: True) allows downloading from GCS\n",
            "            while keeping a different `data_dir` than the public GCS bucket.  So, to\n",
            "            fully bypass GCS, please use `try_gcs=False` and\n",
            "            `download_and_prepare_kwargs={'download_config':\n",
            "            tfds.core.download.DownloadConfig(try_download_gcs=False)})`.\n",
            "          **builder_kwargs: `dict` of keyword arguments passed to the\n",
            "            `tfds.core.DatasetBuilder`.\n",
            "        \n",
            "        Returns:\n",
            "          A `tfds.core.DatasetBuilder`.\n",
            "        \n",
            "        Raises:\n",
            "          DatasetNotFoundError: if `name` is unrecognized.\n",
            "    \n",
            "    builder_cls(name: 'str') -> 'Type[dataset_builder.DatasetBuilder]'\n",
            "        Fetches a `tfds.core.DatasetBuilder` class by string name.\n",
            "        \n",
            "        Args:\n",
            "          name: `str`, the registered name of the `DatasetBuilder` (the class name as\n",
            "            camel or snake case: `MyDataset` or `my_dataset`).\n",
            "        \n",
            "        Returns:\n",
            "          A `tfds.core.DatasetBuilder` class.\n",
            "        \n",
            "        Raises:\n",
            "          DatasetNotFoundError: if `name` is unrecognized.\n",
            "    \n",
            "    builder_from_directories(builder_dirs: List[Union[str, os.PathLike]], *, filetype_suffix: Optional[str] = None) -> tensorflow_datasets.core.dataset_builder.DatasetBuilder\n",
            "        Loads a `tfds.core.DatasetBuilder` from the given generated dataset path.\n",
            "        \n",
            "        When a dataset is spread out over multiple folders, then this function can be\n",
            "        used to easily read from all builder dirs.\n",
            "        \n",
            "        Note that the data in each folder must have the same features, dataset name,\n",
            "        and version.\n",
            "        \n",
            "        Some examples of when a dataset might be spread out over multiple folders:\n",
            "        \n",
            "        - in reinforcement learning, multiple agents each produce a dataset\n",
            "        - each day a new dataset is produced based on new incoming events\n",
            "        \n",
            "        Arguments:\n",
            "          builder_dirs: the list of builder dirs from which the data should be read.\n",
            "          filetype_suffix: DEPRECATED PLEASE DO NOT USE. The filetype suffix (e.g.\n",
            "            'tfrecord') that is used if the file format is not specified in the\n",
            "            DatasetInfo.\n",
            "        \n",
            "        Returns:\n",
            "          the read only dataset builder that is configured to read from all the given\n",
            "          builder dirs.\n",
            "    \n",
            "    builder_from_directory(builder_dir: Union[str, os.PathLike]) -> tensorflow_datasets.core.dataset_builder.DatasetBuilder\n",
            "        Loads a `tfds.core.DatasetBuilder` from the given generated dataset path.\n",
            "        \n",
            "        Reconstructs the `tfds.core.DatasetBuilder` without requiring the original\n",
            "        generation code.\n",
            "        \n",
            "        From `<builder_dir>/features.json` it infers the structure (feature names,\n",
            "        nested dict,...) and content (image, sequence,...) of the dataset. The\n",
            "        serialization format is defined in\n",
            "        `tfds.features.FeatureConnector` in `to_json()`.\n",
            "        \n",
            "        Note: This function only works for datasets generated with TFDS `4.0.0` or\n",
            "        above.\n",
            "        \n",
            "        Args:\n",
            "          builder_dir: `str`, path of the directory containing the dataset to read (\n",
            "            e.g. `~/tensorflow_datasets/mnist/3.0.0/`).\n",
            "        \n",
            "        Returns:\n",
            "          builder: `tfds.core.DatasetBuilder`, builder for dataset at the given path.\n",
            "    \n",
            "    data_source(name: 'str', *, split: 'Optional[Tree[splits_lib.SplitArg]]' = None, data_dir: 'Union[None, str, os.PathLike]' = None, download: 'bool' = True, decoders: 'Optional[TreeDict[decode.partial_decode.DecoderArg]]' = None, builder_kwargs: 'Optional[Dict[str, Any]]' = None, download_and_prepare_kwargs: 'Optional[Dict[str, Any]]' = None, try_gcs: 'bool' = False) -> 'type_utils.ListOrTreeOrElem[Sequence[Any]]'\n",
            "        Gets a data source from the named dataset.\n",
            "        \n",
            "        `tfds.data_source` is a convenience method that:\n",
            "        \n",
            "        1. Fetches the `tfds.core.DatasetBuilder` by name:\n",
            "        \n",
            "           ```python\n",
            "           builder = tfds.builder(name, data_dir=data_dir, **builder_kwargs)\n",
            "           ```\n",
            "        \n",
            "        2. Generates the data (when `download=True`):\n",
            "        \n",
            "           ```python\n",
            "           builder.download_and_prepare(**download_and_prepare_kwargs)\n",
            "           ```\n",
            "        \n",
            "        3. Gets the data source:\n",
            "        \n",
            "           ```python\n",
            "           ds = builder.as_data_source(split=split)\n",
            "           ```\n",
            "        \n",
            "        You can consume data sources:\n",
            "        \n",
            "        - In Python by iterating over them:\n",
            "        \n",
            "        ```python\n",
            "        for example in ds['train']:\n",
            "          print(example)\n",
            "        ```\n",
            "        \n",
            "        - With a DataLoader (e.g., with\n",
            "        [Pytorch](https://pytorch.org/docs/stable/data.html)).\n",
            "        \n",
            "        **Warning**: calling this function might potentially trigger the download\n",
            "        of hundreds of GiB to disk. Refer to the `download` argument.\n",
            "        \n",
            "        Args:\n",
            "          name: `str`, the registered name of the `DatasetBuilder` (the snake case\n",
            "            version of the class name). The config and version can also be specified\n",
            "            in the name as follows: `'dataset_name[/config_name][:version]'`. For\n",
            "            example, `'movielens/25m-ratings'` (for the latest version of\n",
            "            `'25m-ratings'`), `'movielens:0.1.0'` (for the default config and version\n",
            "            0.1.0), or`'movielens/25m-ratings:0.1.0'`. Note that only the latest\n",
            "            version can be generated, but old versions can be read if they are present\n",
            "            on disk. For convenience, the `name` parameter can contain comma-separated\n",
            "            keyword arguments for the builder. For example, `'foo_bar/a=True,b=3'`\n",
            "            would use the `FooBar` dataset passing the keyword arguments `a=True` and\n",
            "            `b=3` (for builders with configs, it would be `'foo_bar/zoo/a=True,b=3'`\n",
            "            to use the `'zoo'` config and pass to the builder keyword arguments\n",
            "            `a=True` and `b=3`).\n",
            "          split: Which split of the data to load (e.g. `'train'`, `'test'`, `['train',\n",
            "            'test']`, `'train[80%:]'`,...). See our [split API\n",
            "            guide](https://www.tensorflow.org/datasets/splits). If `None`, will return\n",
            "            all splits in a `Dict[Split, Sequence]`\n",
            "          data_dir: directory to read/write data. Defaults to the value of the\n",
            "            environment variable TFDS_DATA_DIR, if set, otherwise falls back to\n",
            "            datasets are stored.\n",
            "          download: `bool` (optional), whether to call\n",
            "            `tfds.core.DatasetBuilder.download_and_prepare` before calling\n",
            "            `tfds.core.DatasetBuilder.as_data_source`. If `False`, data is expected to\n",
            "            be in `data_dir`. If `True` and the data is already in `data_dir`,\n",
            "            when data_dir is a Placer path.\n",
            "          decoders: Nested dict of `Decoder` objects which allow to customize the\n",
            "            decoding. The structure should match the feature structure, but only\n",
            "            customized feature keys need to be present. See [the\n",
            "            guide](https://github.com/tensorflow/datasets/blob/master/docs/decode.md)\n",
            "            for more info.\n",
            "          builder_kwargs: `dict` (optional), keyword arguments to be passed to the\n",
            "            `tfds.core.DatasetBuilder` constructor. `data_dir` will be passed through\n",
            "            by default.\n",
            "          download_and_prepare_kwargs: `dict` (optional) keyword arguments passed to\n",
            "            `tfds.core.DatasetBuilder.download_and_prepare` if `download=True`. Allow\n",
            "            to control where to download and extract the cached data. If not set,\n",
            "            cache_dir and manual_dir will automatically be deduced from data_dir.\n",
            "          try_gcs: `bool`, if True, `tfds.load` will see if the dataset exists on the\n",
            "            public GCS bucket before building it locally. This is equivalent to\n",
            "            passing `data_dir='gs://tfds-data/datasets'`. Warning: `try_gcs` is\n",
            "            different than `builder_kwargs.download_config.try_download_gcs`.\n",
            "            `try_gcs` (default: False) overrides `data_dir` to be the public GCS\n",
            "            bucket. `try_download_gcs` (default: True) allows downloading from GCS\n",
            "            while keeping a different `data_dir` than the public GCS bucket.  So, to\n",
            "            fully bypass GCS, please use `try_gcs=False` and\n",
            "            `download_and_prepare_kwargs={'download_config':\n",
            "            tfds.core.download.DownloadConfig(try_download_gcs=False)})`.\n",
            "        \n",
            "        Returns:\n",
            "          `Sequence` if `split`,\n",
            "          `dict<key: tfds.Split, value: Sequence>` otherwise.\n",
            "    \n",
            "    dataset_collection(name: 'str', loader_kwargs: 'Optional[Dict[str, Any]]' = None) -> 'DatasetCollectionLoader'\n",
            "        Instantiates a DatasetCollectionLoader.\n",
            "        \n",
            "        Args:\n",
            "          name: The name of the dataset collection to load.\n",
            "          loader_kwargs: `dict` (optional), keyword arguments to be passed to the\n",
            "            `tfds.load` function. Refer to `tfds.load` documentation for a\n",
            "            comperehensive overview of the different loading options.\n",
            "        \n",
            "        Returns:\n",
            "          A DatasetCollectionLoader object.\n",
            "        \n",
            "        Raises:\n",
            "          DatasetCollectionNotFoundError if dataset collection not found in registry.\n",
            "    \n",
            "    disable_progress_bar()\n",
            "        Disables Tqdm progress bar.\n",
            "        \n",
            "        Usage:\n",
            "        \n",
            "        ```\n",
            "        tfds.disable_progress_bar()\n",
            "        ```\n",
            "    \n",
            "    display_progress_bar(enable: bool) -> None\n",
            "        Controls whether Tqdm progress bar is enabled/disabled.\n",
            "        \n",
            "        Usage:\n",
            "        \n",
            "        ```\n",
            "        tfds.display_progress_bar(enable=True)\n",
            "        ```\n",
            "        \n",
            "        Args:\n",
            "          enable: whether to display the progress bar.\n",
            "    \n",
            "    enable_progress_bar()\n",
            "        Enables Tqdm progress bar.\n",
            "        \n",
            "        Usage:\n",
            "        \n",
            "        ```\n",
            "        tfds.enable_progress_bar()\n",
            "        ```\n",
            "    \n",
            "    even_splits(split: str, n: int, *, drop_remainder: bool = False) -> List[Union[str, ForwardRef('AbstractSplit')]]\n",
            "        Generates a list of non-overlapping sub-splits of same size.\n",
            "        \n",
            "        Example:\n",
            "        \n",
            "        ```python\n",
            "        split0, split1, split2 = tfds.even_splits('train', n=3, drop_remainder=True)\n",
            "        \n",
            "        # Load 1/3 of the train split.\n",
            "        ds = tfds.load('my_dataset', split=split0)\n",
            "        ```\n",
            "        \n",
            "        `tfds.even_splits` supports arbitrary\n",
            "        [sub-splits](https://www.tensorflow.org/datasets/splits) inputs, including\n",
            "        other `tfds.even_splits` outputs.\n",
            "        \n",
            "        Args:\n",
            "          split: Split (e.g. 'train', 'test[75%:]',...)\n",
            "          n: Number of sub-splits to create\n",
            "          drop_remainder: Drop examples if the number of examples in the datasets is\n",
            "            not evenly divisible by `n`. If `False`, examples are distributed evenly\n",
            "            across subsplits, starting by the first. For example, if there is 11\n",
            "            examples with `n=3`, splits will contain `[4, 4, 3]` examples\n",
            "            respectivelly.\n",
            "        \n",
            "        Returns:\n",
            "          The list of subsplits. Those splits can be combined together (with\n",
            "            `+`) or with other subsplits (e.g. `split + 'test[75%:]'`).\n",
            "    \n",
            "    is_dataset_on_gcs(dataset_name: str) -> bool\n",
            "        If the dataset is available on the GCS bucket gs://tfds-data/datasets.\n",
            "    \n",
            "    list_builders(*, with_community_datasets: 'bool' = True) -> 'List[str]'\n",
            "        Returns the string names of all `tfds.core.DatasetBuilder`s.\n",
            "    \n",
            "    list_dataset_collections() -> 'List[str]'\n",
            "        Returns the string names of all `tfds.core.DatasetCollectionBuilder`s.\n",
            "    \n",
            "    load(name: 'str', *, split: 'Optional[Tree[splits_lib.SplitArg]]' = None, data_dir: 'Union[None, str, os.PathLike]' = None, batch_size: 'Optional[int]' = None, shuffle_files: 'bool' = False, download: 'bool' = True, as_supervised: 'bool' = False, decoders: 'Optional[TreeDict[decode.partial_decode.DecoderArg]]' = None, read_config: 'Optional[read_config_lib.ReadConfig]' = None, with_info: 'bool' = False, builder_kwargs: 'Optional[Dict[str, Any]]' = None, download_and_prepare_kwargs: 'Optional[Dict[str, Any]]' = None, as_dataset_kwargs: 'Optional[Dict[str, Any]]' = None, try_gcs: 'bool' = False)\n",
            "        Loads the named dataset into a `tf.data.Dataset`.\n",
            "        \n",
            "        `tfds.load` is a convenience method that:\n",
            "        \n",
            "        1. Fetch the `tfds.core.DatasetBuilder` by name:\n",
            "        \n",
            "           ```python\n",
            "           builder = tfds.builder(name, data_dir=data_dir, **builder_kwargs)\n",
            "           ```\n",
            "        \n",
            "        2. Generate the data (when `download=True`):\n",
            "        \n",
            "           ```python\n",
            "           builder.download_and_prepare(**download_and_prepare_kwargs)\n",
            "           ```\n",
            "        \n",
            "        3. Load the `tf.data.Dataset` object:\n",
            "        \n",
            "           ```python\n",
            "           ds = builder.as_dataset(\n",
            "               split=split,\n",
            "               as_supervised=as_supervised,\n",
            "               shuffle_files=shuffle_files,\n",
            "               read_config=read_config,\n",
            "               decoders=decoders,\n",
            "               **as_dataset_kwargs,\n",
            "           )\n",
            "           ```\n",
            "        \n",
            "        See: https://www.tensorflow.org/datasets/overview#load_a_dataset for more\n",
            "        examples.\n",
            "        \n",
            "        If you'd like NumPy arrays instead of `tf.data.Dataset`s or `tf.Tensor`s,\n",
            "        you can pass the return value to `tfds.as_numpy`.\n",
            "        \n",
            "        **Warning**: calling this function might potentially trigger the download\n",
            "        of hundreds of GiB to disk. Refer to the `download` argument.\n",
            "        \n",
            "        Args:\n",
            "          name: `str`, the registered name of the `DatasetBuilder` (the snake case\n",
            "            version of the class name). The config and version can also be specified\n",
            "            in the name as follows: `'dataset_name[/config_name][:version]'`. For\n",
            "            example, `'movielens/25m-ratings'` (for the latest version of\n",
            "            `'25m-ratings'`), `'movielens:0.1.0'` (for the default config and version\n",
            "            0.1.0), or`'movielens/25m-ratings:0.1.0'`. Note that only the latest\n",
            "            version can be generated, but old versions can be read if they are present\n",
            "            on disk. For convenience, the `name` parameter can contain comma-separated\n",
            "            keyword arguments for the builder. For example, `'foo_bar/a=True,b=3'`\n",
            "            would use the `FooBar` dataset passing the keyword arguments `a=True` and\n",
            "            `b=3` (for builders with configs, it would be `'foo_bar/zoo/a=True,b=3'`\n",
            "            to use the `'zoo'` config and pass to the builder keyword arguments\n",
            "            `a=True` and `b=3`).\n",
            "          split: Which split of the data to load (e.g. `'train'`, `'test'`, `['train',\n",
            "            'test']`, `'train[80%:]'`,...). See our [split API\n",
            "            guide](https://www.tensorflow.org/datasets/splits). If `None`, will return\n",
            "            all splits in a `Dict[Split, tf.data.Dataset]`\n",
            "          data_dir: directory to read/write data. Defaults to the value of the\n",
            "            environment variable TFDS_DATA_DIR, if set, otherwise falls back to\n",
            "            datasets are stored.\n",
            "          batch_size: `int`, if set, add a batch dimension to examples. Note that\n",
            "            variable length features will be 0-padded. If `batch_size=-1`, will return\n",
            "            the full dataset as `tf.Tensor`s.\n",
            "          shuffle_files: `bool`, whether to shuffle the input files. Defaults to\n",
            "            `False`.\n",
            "          download: `bool` (optional), whether to call\n",
            "            `tfds.core.DatasetBuilder.download_and_prepare` before calling\n",
            "            `tfds.core.DatasetBuilder.as_dataset`. If `False`, data is expected to be\n",
            "            in `data_dir`. If `True` and the data is already in `data_dir`,\n",
            "            when data_dir is a Placer path.\n",
            "          as_supervised: `bool`, if `True`, the returned `tf.data.Dataset` will have a\n",
            "            2-tuple structure `(input, label)` according to\n",
            "            `builder.info.supervised_keys`. If `False`, the default, the returned\n",
            "            `tf.data.Dataset` will have a dictionary with all the features.\n",
            "          decoders: Nested dict of `Decoder` objects which allow to customize the\n",
            "            decoding. The structure should match the feature structure, but only\n",
            "            customized feature keys need to be present. See [the\n",
            "            guide](https://github.com/tensorflow/datasets/blob/master/docs/decode.md)\n",
            "            for more info.\n",
            "          read_config: `tfds.ReadConfig`, Additional options to configure the input\n",
            "            pipeline (e.g. seed, num parallel reads,...).\n",
            "          with_info: `bool`, if `True`, `tfds.load` will return the tuple\n",
            "            (`tf.data.Dataset`, `tfds.core.DatasetInfo`), the latter containing the\n",
            "            info associated with the builder.\n",
            "          builder_kwargs: `dict` (optional), keyword arguments to be passed to the\n",
            "            `tfds.core.DatasetBuilder` constructor. `data_dir` will be passed through\n",
            "            by default.\n",
            "          download_and_prepare_kwargs: `dict` (optional) keyword arguments passed to\n",
            "            `tfds.core.DatasetBuilder.download_and_prepare` if `download=True`. Allow\n",
            "            to control where to download and extract the cached data. If not set,\n",
            "            cache_dir and manual_dir will automatically be deduced from data_dir.\n",
            "          as_dataset_kwargs: `dict` (optional), keyword arguments passed to\n",
            "            `tfds.core.DatasetBuilder.as_dataset`.\n",
            "          try_gcs: `bool`, if True, `tfds.load` will see if the dataset exists on the\n",
            "            public GCS bucket before building it locally. This is equivalent to\n",
            "            passing `data_dir='gs://tfds-data/datasets'`. Warning: `try_gcs` is\n",
            "            different than `builder_kwargs.download_config.try_download_gcs`.\n",
            "            `try_gcs` (default: False) overrides `data_dir` to be the public GCS\n",
            "            bucket. `try_download_gcs` (default: True) allows downloading from GCS\n",
            "            while keeping a different `data_dir` than the public GCS bucket.  So, to\n",
            "            fully bypass GCS, please use `try_gcs=False` and\n",
            "            `download_and_prepare_kwargs={'download_config':\n",
            "            tfds.core.download.DownloadConfig(try_download_gcs=False)})`.\n",
            "        \n",
            "        Returns:\n",
            "          ds: `tf.data.Dataset`, the dataset requested, or if `split` is None, a\n",
            "            `dict<key: tfds.Split, value: tf.data.Dataset>`. If `batch_size=-1`,\n",
            "            these will be full datasets as `tf.Tensor`s.\n",
            "          ds_info: `tfds.core.DatasetInfo`, if `with_info` is True, then `tfds.load`\n",
            "            will return a tuple `(ds, ds_info)` containing dataset information\n",
            "            (version, features, splits, num_examples,...). Note that the `ds_info`\n",
            "            object documents the entire dataset, regardless of the `split` requested.\n",
            "            Split-specific information is available in `ds_info.splits`.\n",
            "    \n",
            "    show_examples(ds: '_Dataset', ds_info: 'dataset_info.DatasetInfo', is_batched: 'bool' = False, **options_kwargs: 'Any')\n",
            "        Visualize images (and labels) from an image classification dataset.\n",
            "        \n",
            "        This function is for interactive use (Colab, Jupyter). It displays and return\n",
            "        a plot of (rows*columns) images from a tf.data.Dataset.\n",
            "        \n",
            "        Usage:\n",
            "        ```python\n",
            "        ds, ds_info = tfds.load('cifar10', split='train', with_info=True)\n",
            "        fig = tfds.show_examples(ds, ds_info)\n",
            "        ```\n",
            "        \n",
            "        Args:\n",
            "          ds: `tf.data.Dataset`. The tf.data.Dataset object to visualize. Examples\n",
            "            should not be batched. Examples will be consumed in order until (rows *\n",
            "            cols) are read or the dataset is consumed.\n",
            "          ds_info: The dataset info object to which extract the label and features\n",
            "            info. Available either through `tfds.load('mnist', with_info=True)` or\n",
            "            `tfds.builder('mnist').info`\n",
            "          is_batched: Whether the data is batched.\n",
            "          **options_kwargs: Additional display options, specific to the dataset type\n",
            "            to visualize. Are forwarded to `tfds.visualization.Visualizer.show`. See\n",
            "            the `tfds.visualization` for a list of available visualizers.\n",
            "        \n",
            "        Returns:\n",
            "          fig: The `matplotlib.Figure` object\n",
            "    \n",
            "    show_statistics(ds_info: 'dataset_info.DatasetInfo', split: 'splits.Split' = Split('train'), disable_logging: 'bool' = True) -> 'None'\n",
            "        Display the datasets statistics on a Colab/Jupyter notebook.\n",
            "        \n",
            "        `tfds.show_statistics` is a wrapper around\n",
            "        [tensorflow_data_validation](https://www.tensorflow.org/tfx/data_validation/get_started)\n",
            "        which calls `tfdv.visualize_statistics`. Statistics are displayed using\n",
            "        [FACETS OVERVIEW](https://pair-code.github.io/facets/).\n",
            "        \n",
            "        Usage:\n",
            "        \n",
            "        ```\n",
            "        builder = tfds.builder('mnist')\n",
            "        tfds.show_statistics(builder.info)\n",
            "        ```\n",
            "        \n",
            "        Or:\n",
            "        \n",
            "        ```\n",
            "        ds, ds_info = tfds.load('mnist', with_info)\n",
            "        tfds.show_statistics(ds_info)\n",
            "        ```\n",
            "        \n",
            "        Note: In order to work, `tensorflow_data_validation` must be installed and\n",
            "        the dataset info object must contain the statistics. For \"official\" datasets,\n",
            "        only datasets which have been added/updated recently will contains statistics.\n",
            "        For \"custom\" datasets, you need to generate the dataset with\n",
            "        `tensorflow_data_validation` installed to have the statistics.\n",
            "        \n",
            "        Args:\n",
            "          ds_info: The `tfds.core.DatasetInfo` object containing the statistics.\n",
            "          split: Split for which generate the statistics.\n",
            "          disable_logging: `bool`, if True, disable the tfdv logs which can be too\n",
            "            verbose.\n",
            "        \n",
            "        Returns:\n",
            "          `None`\n",
            "    \n",
            "    split_for_jax_process(split: str, *, process_index: Optional[int] = None, process_count: Optional[int] = None, drop_remainder: bool = False) -> Union[str, ForwardRef('AbstractSplit')]\n",
            "        Returns the subsplit of the data for the process.\n",
            "        \n",
            "        In distributed setting, all process/hosts should get a non-overlapping,\n",
            "        equally sized slice of the entire data. This function takes as input a split\n",
            "        and extracts the slice for the current process index.\n",
            "        \n",
            "        Usage:\n",
            "        \n",
            "        ```python\n",
            "        tfds.load(..., split=tfds.split_for_jax_process('train'))\n",
            "        ```\n",
            "        \n",
            "        This funtion is an alias for:\n",
            "        \n",
            "        ```python\n",
            "        tfds.even_splits(split, n=jax.process_count())[jax.process_index()]\n",
            "        ```\n",
            "        \n",
            "        By default, if examples can't be evenly distributed across processes, you can\n",
            "        drop extra examples with `drop_remainder=True`.\n",
            "        \n",
            "        Args:\n",
            "          split: Split to distribute across host (e.g. `train[75%:]`,\n",
            "            `train[:800]+validation[:100]`).\n",
            "          process_index: Process index in `[0, count)`. Defaults to\n",
            "            `jax.process_index()`.\n",
            "          process_count: Number of processes. Defaults to `jax.process_count()`.\n",
            "          drop_remainder: Drop examples if the number of examples in the datasets is\n",
            "            not evenly divisible by `n`. If `False`, examples are distributed evenly\n",
            "            across subsplits, starting by the first. For example, if there is 11\n",
            "            examples with `n=3`, splits will contain `[4, 4, 3]` examples\n",
            "            respectivelly.\n",
            "        \n",
            "        Returns:\n",
            "          subsplit: The sub-split of the given `split` for the current\n",
            "            `process_index`.\n",
            "\n",
            "DATA\n",
            "    __all__ = ['as_dataframe', 'as_numpy', 'beam', 'benchmark', 'builder',...\n",
            "    testing = LazyModule(module_name='tensorflow_datasets.test...g',), err...\n",
            "\n",
            "VERSION\n",
            "    4.9.4\n",
            "\n",
            "FILE\n",
            "    /usr/local/lib/python3.10/dist-packages/tensorflow_datasets/__init__.py\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "help(tfds.load)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z23zI4f5wLQS",
        "outputId": "d51762dc-d8dd-4a52-c411-0015c27276d2"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Help on function load in module tensorflow_datasets.core.load:\n",
            "\n",
            "load(name: 'str', *, split: 'Optional[Tree[splits_lib.SplitArg]]' = None, data_dir: 'Union[None, str, os.PathLike]' = None, batch_size: 'Optional[int]' = None, shuffle_files: 'bool' = False, download: 'bool' = True, as_supervised: 'bool' = False, decoders: 'Optional[TreeDict[decode.partial_decode.DecoderArg]]' = None, read_config: 'Optional[read_config_lib.ReadConfig]' = None, with_info: 'bool' = False, builder_kwargs: 'Optional[Dict[str, Any]]' = None, download_and_prepare_kwargs: 'Optional[Dict[str, Any]]' = None, as_dataset_kwargs: 'Optional[Dict[str, Any]]' = None, try_gcs: 'bool' = False)\n",
            "    Loads the named dataset into a `tf.data.Dataset`.\n",
            "    \n",
            "    `tfds.load` is a convenience method that:\n",
            "    \n",
            "    1. Fetch the `tfds.core.DatasetBuilder` by name:\n",
            "    \n",
            "       ```python\n",
            "       builder = tfds.builder(name, data_dir=data_dir, **builder_kwargs)\n",
            "       ```\n",
            "    \n",
            "    2. Generate the data (when `download=True`):\n",
            "    \n",
            "       ```python\n",
            "       builder.download_and_prepare(**download_and_prepare_kwargs)\n",
            "       ```\n",
            "    \n",
            "    3. Load the `tf.data.Dataset` object:\n",
            "    \n",
            "       ```python\n",
            "       ds = builder.as_dataset(\n",
            "           split=split,\n",
            "           as_supervised=as_supervised,\n",
            "           shuffle_files=shuffle_files,\n",
            "           read_config=read_config,\n",
            "           decoders=decoders,\n",
            "           **as_dataset_kwargs,\n",
            "       )\n",
            "       ```\n",
            "    \n",
            "    See: https://www.tensorflow.org/datasets/overview#load_a_dataset for more\n",
            "    examples.\n",
            "    \n",
            "    If you'd like NumPy arrays instead of `tf.data.Dataset`s or `tf.Tensor`s,\n",
            "    you can pass the return value to `tfds.as_numpy`.\n",
            "    \n",
            "    **Warning**: calling this function might potentially trigger the download\n",
            "    of hundreds of GiB to disk. Refer to the `download` argument.\n",
            "    \n",
            "    Args:\n",
            "      name: `str`, the registered name of the `DatasetBuilder` (the snake case\n",
            "        version of the class name). The config and version can also be specified\n",
            "        in the name as follows: `'dataset_name[/config_name][:version]'`. For\n",
            "        example, `'movielens/25m-ratings'` (for the latest version of\n",
            "        `'25m-ratings'`), `'movielens:0.1.0'` (for the default config and version\n",
            "        0.1.0), or`'movielens/25m-ratings:0.1.0'`. Note that only the latest\n",
            "        version can be generated, but old versions can be read if they are present\n",
            "        on disk. For convenience, the `name` parameter can contain comma-separated\n",
            "        keyword arguments for the builder. For example, `'foo_bar/a=True,b=3'`\n",
            "        would use the `FooBar` dataset passing the keyword arguments `a=True` and\n",
            "        `b=3` (for builders with configs, it would be `'foo_bar/zoo/a=True,b=3'`\n",
            "        to use the `'zoo'` config and pass to the builder keyword arguments\n",
            "        `a=True` and `b=3`).\n",
            "      split: Which split of the data to load (e.g. `'train'`, `'test'`, `['train',\n",
            "        'test']`, `'train[80%:]'`,...). See our [split API\n",
            "        guide](https://www.tensorflow.org/datasets/splits). If `None`, will return\n",
            "        all splits in a `Dict[Split, tf.data.Dataset]`\n",
            "      data_dir: directory to read/write data. Defaults to the value of the\n",
            "        environment variable TFDS_DATA_DIR, if set, otherwise falls back to\n",
            "        datasets are stored.\n",
            "      batch_size: `int`, if set, add a batch dimension to examples. Note that\n",
            "        variable length features will be 0-padded. If `batch_size=-1`, will return\n",
            "        the full dataset as `tf.Tensor`s.\n",
            "      shuffle_files: `bool`, whether to shuffle the input files. Defaults to\n",
            "        `False`.\n",
            "      download: `bool` (optional), whether to call\n",
            "        `tfds.core.DatasetBuilder.download_and_prepare` before calling\n",
            "        `tfds.core.DatasetBuilder.as_dataset`. If `False`, data is expected to be\n",
            "        in `data_dir`. If `True` and the data is already in `data_dir`,\n",
            "        when data_dir is a Placer path.\n",
            "      as_supervised: `bool`, if `True`, the returned `tf.data.Dataset` will have a\n",
            "        2-tuple structure `(input, label)` according to\n",
            "        `builder.info.supervised_keys`. If `False`, the default, the returned\n",
            "        `tf.data.Dataset` will have a dictionary with all the features.\n",
            "      decoders: Nested dict of `Decoder` objects which allow to customize the\n",
            "        decoding. The structure should match the feature structure, but only\n",
            "        customized feature keys need to be present. See [the\n",
            "        guide](https://github.com/tensorflow/datasets/blob/master/docs/decode.md)\n",
            "        for more info.\n",
            "      read_config: `tfds.ReadConfig`, Additional options to configure the input\n",
            "        pipeline (e.g. seed, num parallel reads,...).\n",
            "      with_info: `bool`, if `True`, `tfds.load` will return the tuple\n",
            "        (`tf.data.Dataset`, `tfds.core.DatasetInfo`), the latter containing the\n",
            "        info associated with the builder.\n",
            "      builder_kwargs: `dict` (optional), keyword arguments to be passed to the\n",
            "        `tfds.core.DatasetBuilder` constructor. `data_dir` will be passed through\n",
            "        by default.\n",
            "      download_and_prepare_kwargs: `dict` (optional) keyword arguments passed to\n",
            "        `tfds.core.DatasetBuilder.download_and_prepare` if `download=True`. Allow\n",
            "        to control where to download and extract the cached data. If not set,\n",
            "        cache_dir and manual_dir will automatically be deduced from data_dir.\n",
            "      as_dataset_kwargs: `dict` (optional), keyword arguments passed to\n",
            "        `tfds.core.DatasetBuilder.as_dataset`.\n",
            "      try_gcs: `bool`, if True, `tfds.load` will see if the dataset exists on the\n",
            "        public GCS bucket before building it locally. This is equivalent to\n",
            "        passing `data_dir='gs://tfds-data/datasets'`. Warning: `try_gcs` is\n",
            "        different than `builder_kwargs.download_config.try_download_gcs`.\n",
            "        `try_gcs` (default: False) overrides `data_dir` to be the public GCS\n",
            "        bucket. `try_download_gcs` (default: True) allows downloading from GCS\n",
            "        while keeping a different `data_dir` than the public GCS bucket.  So, to\n",
            "        fully bypass GCS, please use `try_gcs=False` and\n",
            "        `download_and_prepare_kwargs={'download_config':\n",
            "        tfds.core.download.DownloadConfig(try_download_gcs=False)})`.\n",
            "    \n",
            "    Returns:\n",
            "      ds: `tf.data.Dataset`, the dataset requested, or if `split` is None, a\n",
            "        `dict<key: tfds.Split, value: tf.data.Dataset>`. If `batch_size=-1`,\n",
            "        these will be full datasets as `tf.Tensor`s.\n",
            "      ds_info: `tfds.core.DatasetInfo`, if `with_info` is True, then `tfds.load`\n",
            "        will return a tuple `(ds, ds_info)` containing dataset information\n",
            "        (version, features, splits, num_examples,...). Note that the `ds_info`\n",
            "        object documents the entire dataset, regardless of the `split` requested.\n",
            "        Split-specific information is available in `ds_info.splits`.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_norm(image, label):\n",
        "  image = tf.image.resize(image, [299, 299])\n",
        "  image = image / 127.5 - 1\n",
        "  return image, label\n",
        "\n",
        "norm_train = train_dataset.map(preprocess_norm).batch(32).prefetch(tf.data.experimental.AUTOTUNE)\n",
        "norm_val = val_dataset.map(preprocess_norm).batch(32).prefetch(tf.data.experimental.AUTOTUNE)"
      ],
      "metadata": {
        "id": "OQhPWL1sHQk-"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preprocess_input"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "id": "Om4g3y-SH1YI",
        "outputId": "cf00028b-3b4b-45f8-cd1e-628bb66c4674"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<function keras.src.applications.mobilenet.preprocess_input(x, data_format=None)>"
            ],
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>keras.src.applications.mobilenet.preprocess_input</b><br/>def preprocess_input(x, data_format=None)</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/usr/local/lib/python3.10/dist-packages/keras/src/applications/mobilenet.py</a>Preprocesses a tensor or Numpy array encoding a batch of images.\n",
              "\n",
              "Usage example with `applications.MobileNet`:\n",
              "\n",
              "```python\n",
              "i = tf.keras.layers.Input([None, None, 3], dtype = tf.uint8)\n",
              "x = tf.cast(i, tf.float32)\n",
              "x = tf.keras.applications.mobilenet.preprocess_input(x)\n",
              "core = tf.keras.applications.MobileNet()\n",
              "x = core(x)\n",
              "model = tf.keras.Model(inputs=[i], outputs=[x])\n",
              "\n",
              "image = tf.image.decode_png(tf.io.read_file(&#x27;file.png&#x27;))\n",
              "result = model(image)\n",
              "```\n",
              "\n",
              "Args:\n",
              "  x: A floating point `numpy.array` or a `tf.Tensor`, 3D or 4D with 3 color\n",
              "    channels, with values in the range [0, 255].\n",
              "    The preprocessed data are written over the input data\n",
              "    if the data types are compatible. To avoid this\n",
              "    behaviour, `numpy.copy(x)` can be used.\n",
              "  data_format: Optional data format of the image tensor/array. None, means\n",
              "    the global setting `tf.keras.backend.image_data_format()` is used\n",
              "    (unless you changed it, it uses &quot;channels_last&quot;).\n",
              "    Defaults to `None`.\n",
              "\n",
              "Returns:\n",
              "    Preprocessed `numpy.array` or a `tf.Tensor` with type `float32`.\n",
              "    \n",
              "    The inputs pixel values are scaled between -1 and 1, sample-wise.\n",
              "\n",
              "Raises:\n",
              "    \n",
              "  ValueError: In case of unknown `data_format` argument.</pre>\n",
              "      <script>\n",
              "      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n",
              "        for (const element of document.querySelectorAll('.filepath')) {\n",
              "          element.style.display = 'block'\n",
              "          element.onclick = (event) => {\n",
              "            event.preventDefault();\n",
              "            event.stopPropagation();\n",
              "            google.colab.files.view(element.textContent, 472);\n",
              "          };\n",
              "        }\n",
              "      }\n",
              "      for (const element of document.querySelectorAll('.function-repr-contents')) {\n",
              "        element.onclick = (event) => {\n",
              "          event.preventDefault();\n",
              "          event.stopPropagation();\n",
              "          element.classList.toggle('function-repr-contents-collapsed');\n",
              "        };\n",
              "      }\n",
              "      </script>\n",
              "      </div>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "norm_train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C5K1c138CvIB",
        "outputId": "7ba8bd22-544d-445b-9ec0-0da668c794c9"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<_PrefetchDataset element_spec=(TensorSpec(shape=(None, 299, 299, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None,), dtype=tf.int64, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#when calling a function and inputting arguments CTRL + SPACE will display which arguments are available"
      ],
      "metadata": {
        "id": "pjWyDTvPLJiE"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for layer in dir(tf.keras.layers):\n",
        "  print(layer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QU4Zk5juRDyB",
        "outputId": "2f5697c8-b795-460d-d2aa-1a4478a5d482"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AbstractRNNCell\n",
            "Activation\n",
            "ActivityRegularization\n",
            "Add\n",
            "AdditiveAttention\n",
            "AlphaDropout\n",
            "Attention\n",
            "Average\n",
            "AveragePooling1D\n",
            "AveragePooling2D\n",
            "AveragePooling3D\n",
            "AvgPool1D\n",
            "AvgPool2D\n",
            "AvgPool3D\n",
            "BatchNormalization\n",
            "Bidirectional\n",
            "CategoryEncoding\n",
            "CenterCrop\n",
            "Concatenate\n",
            "Conv1D\n",
            "Conv1DTranspose\n",
            "Conv2D\n",
            "Conv2DTranspose\n",
            "Conv3D\n",
            "Conv3DTranspose\n",
            "ConvLSTM1D\n",
            "ConvLSTM2D\n",
            "ConvLSTM3D\n",
            "Convolution1D\n",
            "Convolution1DTranspose\n",
            "Convolution2D\n",
            "Convolution2DTranspose\n",
            "Convolution3D\n",
            "Convolution3DTranspose\n",
            "Cropping1D\n",
            "Cropping2D\n",
            "Cropping3D\n",
            "Dense\n",
            "DenseFeatures\n",
            "DepthwiseConv1D\n",
            "DepthwiseConv2D\n",
            "Discretization\n",
            "Dot\n",
            "Dropout\n",
            "ELU\n",
            "EinsumDense\n",
            "Embedding\n",
            "Flatten\n",
            "GRU\n",
            "GRUCell\n",
            "GaussianDropout\n",
            "GaussianNoise\n",
            "GlobalAveragePooling1D\n",
            "GlobalAveragePooling2D\n",
            "GlobalAveragePooling3D\n",
            "GlobalAvgPool1D\n",
            "GlobalAvgPool2D\n",
            "GlobalAvgPool3D\n",
            "GlobalMaxPool1D\n",
            "GlobalMaxPool2D\n",
            "GlobalMaxPool3D\n",
            "GlobalMaxPooling1D\n",
            "GlobalMaxPooling2D\n",
            "GlobalMaxPooling3D\n",
            "GroupNormalization\n",
            "HashedCrossing\n",
            "Hashing\n",
            "Identity\n",
            "Input\n",
            "InputLayer\n",
            "InputSpec\n",
            "IntegerLookup\n",
            "LSTM\n",
            "LSTMCell\n",
            "Lambda\n",
            "Layer\n",
            "LayerNormalization\n",
            "LeakyReLU\n",
            "LocallyConnected1D\n",
            "LocallyConnected2D\n",
            "Masking\n",
            "MaxPool1D\n",
            "MaxPool2D\n",
            "MaxPool3D\n",
            "MaxPooling1D\n",
            "MaxPooling2D\n",
            "MaxPooling3D\n",
            "Maximum\n",
            "Minimum\n",
            "MultiHeadAttention\n",
            "Multiply\n",
            "Normalization\n",
            "PReLU\n",
            "Permute\n",
            "RNN\n",
            "RandomBrightness\n",
            "RandomContrast\n",
            "RandomCrop\n",
            "RandomFlip\n",
            "RandomHeight\n",
            "RandomRotation\n",
            "RandomTranslation\n",
            "RandomWidth\n",
            "RandomZoom\n",
            "ReLU\n",
            "RepeatVector\n",
            "Rescaling\n",
            "Reshape\n",
            "Resizing\n",
            "SeparableConv1D\n",
            "SeparableConv2D\n",
            "SeparableConvolution1D\n",
            "SeparableConvolution2D\n",
            "SimpleRNN\n",
            "SimpleRNNCell\n",
            "Softmax\n",
            "SpatialDropout1D\n",
            "SpatialDropout2D\n",
            "SpatialDropout3D\n",
            "SpectralNormalization\n",
            "StackedRNNCells\n",
            "StringLookup\n",
            "Subtract\n",
            "TextVectorization\n",
            "ThresholdedReLU\n",
            "TimeDistributed\n",
            "UnitNormalization\n",
            "UpSampling1D\n",
            "UpSampling2D\n",
            "UpSampling3D\n",
            "Wrapper\n",
            "ZeroPadding1D\n",
            "ZeroPadding2D\n",
            "ZeroPadding3D\n",
            "__builtins__\n",
            "__cached__\n",
            "__doc__\n",
            "__file__\n",
            "__loader__\n",
            "__name__\n",
            "__package__\n",
            "__path__\n",
            "__spec__\n",
            "add\n",
            "average\n",
            "concatenate\n",
            "deserialize\n",
            "dot\n",
            "experimental\n",
            "maximum\n",
            "minimum\n",
            "multiply\n",
            "serialize\n",
            "subtract\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS=1\n",
        "\n",
        "model.fit(norm_train, epochs=EPOCHS, verbose=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3K5lNF7KRKY2",
        "outputId": "8690b476-a569-4a23-d884-a2d10e258996"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "582/582 [==============================] - 498s 838ms/step - loss: 0.2349 - acc: 0.9074\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7fb4eb027400>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for data in norm_val.take(1):\n",
        "  images, labels = data[0], data[1]\n",
        "  x = np.expand_dims(images[0].numpy(), axis=0)\n",
        "  pred = model.predict(x)\n",
        "  print(pred[0], labels[0])\n",
        "  x = np.expand_dims(images[1].numpy(), axis=0)\n",
        "  pred = model.predict(x)\n",
        "  print(pred[0], labels[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZSeII_3rUt-B",
        "outputId": "b6907355-aa43-4323-bc1a-1699d2e6acd2"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 2s 2s/step\n",
            "[0.9999937] tf.Tensor(1, shape=(), dtype=int64)\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "[0.00028549] tf.Tensor(0, shape=(), dtype=int64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# attempted operation maxes out system RAM\n",
        "#norm_train_list = list(norm_train.as_numpy_iterator())\n",
        "#norm_train_list[0]"
      ],
      "metadata": {
        "id": "OQw5JwWLZfzW"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AlIcIS_eaQSr"
      },
      "execution_count": 15,
      "outputs": []
    }
  ]
}